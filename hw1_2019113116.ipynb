{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrHFVX779yMs",
        "outputId": "9b0991fc-78ee-472b-c7ef-193b0b414079"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#gpu 사용 가능 확인용\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cifar-10 이미지 데이터셋을 다운받고 분류하는 코드를 구현하세요."
      ],
      "metadata": {
        "id": "oGdFbznpmJ5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 데이터 전처리 및 데이터 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# CIFAR-10 데이터셋 로드\n",
        "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# 데이터셋으로 DataLoader 생성\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(cifar_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "''' Image 확인용\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "'''\n",
        "\n",
        "#신경망 정의\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "        self.drop1 = nn.Dropout()\n",
        "        self.fc2 = nn.Linear(512 * 2 * 2, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 모델, 손실 함수, 최적화 함수 정의\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "summary(model, input_size=(3, 32, 32)) # Summary 출력\n",
        "\n",
        "# 훈련\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "        new_labels = [[0.0 for tt in range(10)] for t in range(len(labels))] # 0 0 0 1 0 0 0 0 0 0 형태로 만들어주기 위한 배열\n",
        "        for t in range(len(labels)):\n",
        "          new_labels[t][labels[t]] = 1.0\n",
        "        new_labels = torch.tensor(new_labels)\n",
        "        labels = labels.to(device)\n",
        "        new_labels = new_labels.to(device)\n",
        "        prediction = prediction.to(device)\n",
        "        loss = criterion(outputs, new_labels) #loss 계산(BCEWithLogitsLoss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total += (prediction == prediction).sum().item()\n",
        "        correct += (prediction == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {correct / total * 100:.2f}%') # Loss, Accuracy 출력\n",
        "\n",
        "print('훈련 완료')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRI7bfJ9Xjrq",
        "outputId": "355bab3e-d4b0-4fd2-8410-4da6725711e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13686070.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 30, 30]             896\n",
            "         MaxPool2d-2           [-1, 32, 15, 15]               0\n",
            "            Conv2d-3           [-1, 64, 13, 13]          18,496\n",
            "         MaxPool2d-4             [-1, 64, 6, 6]               0\n",
            "            Conv2d-5            [-1, 128, 4, 4]          73,856\n",
            "            Linear-6                  [-1, 128]         262,272\n",
            "           Dropout-7                  [-1, 128]               0\n",
            "            Linear-8                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 356,810\n",
            "Trainable params: 356,810\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.39\n",
            "Params size (MB): 1.36\n",
            "Estimated Total Size (MB): 1.77\n",
            "----------------------------------------------------------------\n",
            "Epoch [1/50], Loss: 0.2393, Accuracy: 28.00%\n",
            "Epoch [2/50], Loss: 0.2317, Accuracy: 45.17%\n",
            "Epoch [3/50], Loss: 0.2405, Accuracy: 52.84%\n",
            "Epoch [4/50], Loss: 0.1978, Accuracy: 57.74%\n",
            "Epoch [5/50], Loss: 0.2540, Accuracy: 61.20%\n",
            "Epoch [6/50], Loss: 0.1045, Accuracy: 64.26%\n",
            "Epoch [7/50], Loss: 0.2023, Accuracy: 66.72%\n",
            "Epoch [8/50], Loss: 0.2246, Accuracy: 68.44%\n",
            "Epoch [9/50], Loss: 0.1576, Accuracy: 70.27%\n",
            "Epoch [10/50], Loss: 0.0573, Accuracy: 71.40%\n",
            "Epoch [11/50], Loss: 0.1208, Accuracy: 72.58%\n",
            "Epoch [12/50], Loss: 0.1722, Accuracy: 73.83%\n",
            "Epoch [13/50], Loss: 0.0498, Accuracy: 74.81%\n",
            "Epoch [14/50], Loss: 0.2021, Accuracy: 75.64%\n",
            "Epoch [15/50], Loss: 0.2050, Accuracy: 76.23%\n",
            "Epoch [16/50], Loss: 0.0645, Accuracy: 77.11%\n",
            "Epoch [17/50], Loss: 0.1342, Accuracy: 77.62%\n",
            "Epoch [18/50], Loss: 0.0553, Accuracy: 78.94%\n",
            "Epoch [19/50], Loss: 0.0704, Accuracy: 79.16%\n",
            "Epoch [20/50], Loss: 0.1250, Accuracy: 79.82%\n",
            "Epoch [21/50], Loss: 0.0961, Accuracy: 80.38%\n",
            "Epoch [22/50], Loss: 0.0672, Accuracy: 80.98%\n",
            "Epoch [23/50], Loss: 0.1051, Accuracy: 81.41%\n",
            "Epoch [24/50], Loss: 0.0853, Accuracy: 81.68%\n",
            "Epoch [25/50], Loss: 0.1417, Accuracy: 82.45%\n",
            "Epoch [26/50], Loss: 0.0252, Accuracy: 82.85%\n",
            "Epoch [27/50], Loss: 0.0991, Accuracy: 83.23%\n",
            "Epoch [28/50], Loss: 0.1090, Accuracy: 83.77%\n",
            "Epoch [29/50], Loss: 0.0936, Accuracy: 84.00%\n",
            "Epoch [30/50], Loss: 0.1528, Accuracy: 84.38%\n",
            "Epoch [31/50], Loss: 0.0947, Accuracy: 85.10%\n",
            "Epoch [32/50], Loss: 0.0752, Accuracy: 85.39%\n",
            "Epoch [33/50], Loss: 0.0541, Accuracy: 85.52%\n",
            "Epoch [34/50], Loss: 0.1111, Accuracy: 86.06%\n",
            "Epoch [35/50], Loss: 0.0620, Accuracy: 86.35%\n",
            "Epoch [36/50], Loss: 0.0823, Accuracy: 86.64%\n",
            "Epoch [37/50], Loss: 0.0623, Accuracy: 87.13%\n",
            "Epoch [38/50], Loss: 0.0833, Accuracy: 87.19%\n",
            "Epoch [39/50], Loss: 0.0608, Accuracy: 87.30%\n",
            "Epoch [40/50], Loss: 0.0113, Accuracy: 87.85%\n",
            "Epoch [41/50], Loss: 0.0644, Accuracy: 87.98%\n",
            "Epoch [42/50], Loss: 0.0158, Accuracy: 88.08%\n",
            "Epoch [43/50], Loss: 0.0563, Accuracy: 88.32%\n",
            "Epoch [44/50], Loss: 0.1044, Accuracy: 88.77%\n",
            "Epoch [45/50], Loss: 0.0237, Accuracy: 88.72%\n",
            "Epoch [46/50], Loss: 0.0747, Accuracy: 89.28%\n",
            "Epoch [47/50], Loss: 0.0728, Accuracy: 89.02%\n",
            "Epoch [48/50], Loss: 0.0318, Accuracy: 89.18%\n",
            "Epoch [49/50], Loss: 0.0568, Accuracy: 89.77%\n",
            "Epoch [50/50], Loss: 0.0651, Accuracy: 89.43%\n",
            "훈련 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트\n",
        "model.eval().to(device)\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "        new_labels = [[0.0 for tt in range(10)] for t in range(len(labels))]\n",
        "        for t in range(len(labels)):\n",
        "          new_labels[t][labels[t]] = 1.0\n",
        "        new_labels = torch.tensor(new_labels)\n",
        "        new_labels = new_labels.to(device)\n",
        "        labels = labels.to(device)\n",
        "        prediction = prediction.to(device)\n",
        "        loss = criterion(outputs, new_labels)\n",
        "        total += (prediction == prediction).sum().item()\n",
        "        correct += (prediction == labels).sum().item()\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy: {(correct / total * 100):.2f}%') # Loss, Accuracy\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%') # Accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGcH6Z-4XnH9",
        "outputId": "5c4a66fc-8024-4a69-867e-d1ca0e94ad6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0404 Accuracy: 92.19%\n",
            "Loss: 0.0298 Accuracy: 93.75%\n",
            "Loss: 0.0331 Accuracy: 93.75%\n",
            "Loss: 0.0223 Accuracy: 94.53%\n",
            "Loss: 0.0221 Accuracy: 95.31%\n",
            "Loss: 0.0291 Accuracy: 95.57%\n",
            "Loss: 0.0516 Accuracy: 94.87%\n",
            "Loss: 0.0335 Accuracy: 94.73%\n",
            "Loss: 0.0204 Accuracy: 95.14%\n",
            "Loss: 0.0451 Accuracy: 95.00%\n",
            "Loss: 0.0379 Accuracy: 95.17%\n",
            "Loss: 0.0327 Accuracy: 94.92%\n",
            "Loss: 0.0397 Accuracy: 94.95%\n",
            "Loss: 0.0272 Accuracy: 95.20%\n",
            "Loss: 0.0310 Accuracy: 95.31%\n",
            "Loss: 0.0215 Accuracy: 95.51%\n",
            "Loss: 0.0215 Accuracy: 95.68%\n",
            "Loss: 0.0350 Accuracy: 95.49%\n",
            "Loss: 0.0338 Accuracy: 95.31%\n",
            "Loss: 0.0283 Accuracy: 95.31%\n",
            "Loss: 0.0469 Accuracy: 95.16%\n",
            "Loss: 0.0314 Accuracy: 95.10%\n",
            "Loss: 0.0342 Accuracy: 95.11%\n",
            "Loss: 0.0270 Accuracy: 95.05%\n",
            "Loss: 0.0439 Accuracy: 94.94%\n",
            "Loss: 0.0335 Accuracy: 95.01%\n",
            "Loss: 0.0272 Accuracy: 94.97%\n",
            "Loss: 0.0230 Accuracy: 95.09%\n",
            "Loss: 0.0169 Accuracy: 95.20%\n",
            "Loss: 0.0154 Accuracy: 95.26%\n",
            "Loss: 0.0299 Accuracy: 95.26%\n",
            "Loss: 0.0329 Accuracy: 95.21%\n",
            "Loss: 0.0152 Accuracy: 95.27%\n",
            "Loss: 0.0194 Accuracy: 95.31%\n",
            "Loss: 0.0237 Accuracy: 95.36%\n",
            "Loss: 0.0383 Accuracy: 95.36%\n",
            "Loss: 0.0268 Accuracy: 95.31%\n",
            "Loss: 0.0220 Accuracy: 95.35%\n",
            "Loss: 0.0264 Accuracy: 95.35%\n",
            "Loss: 0.0223 Accuracy: 95.31%\n",
            "Loss: 0.0367 Accuracy: 95.31%\n",
            "Loss: 0.0444 Accuracy: 95.28%\n",
            "Loss: 0.0341 Accuracy: 95.28%\n",
            "Loss: 0.0219 Accuracy: 95.31%\n",
            "Loss: 0.0307 Accuracy: 95.35%\n",
            "Loss: 0.0240 Accuracy: 95.38%\n",
            "Loss: 0.0250 Accuracy: 95.35%\n",
            "Loss: 0.0245 Accuracy: 95.35%\n",
            "Loss: 0.0399 Accuracy: 95.28%\n",
            "Loss: 0.0274 Accuracy: 95.31%\n",
            "Loss: 0.0295 Accuracy: 95.34%\n",
            "Loss: 0.0248 Accuracy: 95.40%\n",
            "Loss: 0.0276 Accuracy: 95.43%\n",
            "Loss: 0.0317 Accuracy: 95.40%\n",
            "Loss: 0.0408 Accuracy: 95.26%\n",
            "Loss: 0.0253 Accuracy: 95.28%\n",
            "Loss: 0.0388 Accuracy: 95.20%\n",
            "Loss: 0.0293 Accuracy: 95.26%\n",
            "Loss: 0.0368 Accuracy: 95.21%\n",
            "Loss: 0.0360 Accuracy: 95.13%\n",
            "Loss: 0.0274 Accuracy: 95.13%\n",
            "Loss: 0.0461 Accuracy: 95.04%\n",
            "Loss: 0.0335 Accuracy: 95.06%\n",
            "Loss: 0.0355 Accuracy: 95.00%\n",
            "Loss: 0.0217 Accuracy: 95.02%\n",
            "Loss: 0.0247 Accuracy: 95.03%\n",
            "Loss: 0.0431 Accuracy: 95.01%\n",
            "Loss: 0.0300 Accuracy: 95.04%\n",
            "Loss: 0.0260 Accuracy: 95.06%\n",
            "Loss: 0.0458 Accuracy: 95.04%\n",
            "Loss: 0.0224 Accuracy: 95.09%\n",
            "Loss: 0.0344 Accuracy: 95.05%\n",
            "Loss: 0.0340 Accuracy: 95.01%\n",
            "Loss: 0.0249 Accuracy: 95.00%\n",
            "Loss: 0.0166 Accuracy: 95.06%\n",
            "Loss: 0.0238 Accuracy: 95.09%\n",
            "Loss: 0.0269 Accuracy: 95.11%\n",
            "Loss: 0.0226 Accuracy: 95.13%\n",
            "Loss: 0.0365 Accuracy: 95.09%\n",
            "Loss: 0.0319 Accuracy: 95.10%\n",
            "Loss: 0.0161 Accuracy: 95.16%\n",
            "Loss: 0.0354 Accuracy: 95.14%\n",
            "Loss: 0.0373 Accuracy: 95.11%\n",
            "Loss: 0.0250 Accuracy: 95.13%\n",
            "Loss: 0.0362 Accuracy: 95.11%\n",
            "Loss: 0.0300 Accuracy: 95.13%\n",
            "Loss: 0.0445 Accuracy: 95.15%\n",
            "Loss: 0.0421 Accuracy: 95.17%\n",
            "Loss: 0.0415 Accuracy: 95.10%\n",
            "Loss: 0.0330 Accuracy: 95.10%\n",
            "Loss: 0.0279 Accuracy: 95.09%\n",
            "Loss: 0.0298 Accuracy: 95.09%\n",
            "Loss: 0.0427 Accuracy: 95.06%\n",
            "Loss: 0.0248 Accuracy: 95.08%\n",
            "Loss: 0.0322 Accuracy: 95.08%\n",
            "Loss: 0.0316 Accuracy: 95.08%\n",
            "Loss: 0.0272 Accuracy: 95.09%\n",
            "Loss: 0.0269 Accuracy: 95.11%\n",
            "Loss: 0.0284 Accuracy: 95.12%\n",
            "Loss: 0.0189 Accuracy: 95.16%\n",
            "Loss: 0.0320 Accuracy: 95.16%\n",
            "Loss: 0.0355 Accuracy: 95.13%\n",
            "Loss: 0.0313 Accuracy: 95.15%\n",
            "Loss: 0.0285 Accuracy: 95.18%\n",
            "Loss: 0.0204 Accuracy: 95.18%\n",
            "Loss: 0.0307 Accuracy: 95.18%\n",
            "Loss: 0.0200 Accuracy: 95.21%\n",
            "Loss: 0.0191 Accuracy: 95.24%\n",
            "Loss: 0.0293 Accuracy: 95.24%\n",
            "Loss: 0.0309 Accuracy: 95.23%\n",
            "Loss: 0.0319 Accuracy: 95.23%\n",
            "Loss: 0.0211 Accuracy: 95.23%\n",
            "Loss: 0.0178 Accuracy: 95.27%\n",
            "Loss: 0.0379 Accuracy: 95.26%\n",
            "Loss: 0.0280 Accuracy: 95.26%\n",
            "Loss: 0.0212 Accuracy: 95.29%\n",
            "Loss: 0.0294 Accuracy: 95.30%\n",
            "Loss: 0.0262 Accuracy: 95.30%\n",
            "Loss: 0.0227 Accuracy: 95.31%\n",
            "Loss: 0.0231 Accuracy: 95.33%\n",
            "Loss: 0.0421 Accuracy: 95.31%\n",
            "Loss: 0.0317 Accuracy: 95.31%\n",
            "Loss: 0.0309 Accuracy: 95.34%\n",
            "Loss: 0.0161 Accuracy: 95.38%\n",
            "Loss: 0.0240 Accuracy: 95.38%\n",
            "Loss: 0.0190 Accuracy: 95.40%\n",
            "Loss: 0.0168 Accuracy: 95.44%\n",
            "Loss: 0.0216 Accuracy: 95.45%\n",
            "Loss: 0.0157 Accuracy: 95.48%\n",
            "Loss: 0.0266 Accuracy: 95.49%\n",
            "Loss: 0.0541 Accuracy: 95.44%\n",
            "Loss: 0.0189 Accuracy: 95.45%\n",
            "Loss: 0.0267 Accuracy: 95.45%\n",
            "Loss: 0.0266 Accuracy: 95.45%\n",
            "Loss: 0.0315 Accuracy: 95.44%\n",
            "Loss: 0.0454 Accuracy: 95.39%\n",
            "Loss: 0.0366 Accuracy: 95.38%\n",
            "Loss: 0.0272 Accuracy: 95.39%\n",
            "Loss: 0.0350 Accuracy: 95.37%\n",
            "Loss: 0.0337 Accuracy: 95.36%\n",
            "Loss: 0.0329 Accuracy: 95.36%\n",
            "Loss: 0.0212 Accuracy: 95.38%\n",
            "Loss: 0.0388 Accuracy: 95.35%\n",
            "Loss: 0.0299 Accuracy: 95.38%\n",
            "Loss: 0.0362 Accuracy: 95.40%\n",
            "Loss: 0.0286 Accuracy: 95.42%\n",
            "Loss: 0.0355 Accuracy: 95.42%\n",
            "Loss: 0.0416 Accuracy: 95.40%\n",
            "Loss: 0.0373 Accuracy: 95.38%\n",
            "Loss: 0.0195 Accuracy: 95.39%\n",
            "Loss: 0.0362 Accuracy: 95.37%\n",
            "Loss: 0.0440 Accuracy: 95.36%\n",
            "Loss: 0.0323 Accuracy: 95.33%\n",
            "Loss: 0.0117 Accuracy: 95.36%\n",
            "Loss: 0.0318 Accuracy: 95.36%\n",
            "Loss: 0.0356 Accuracy: 95.36%\n",
            "Loss: 0.0239 Accuracy: 95.37%\n",
            "Loss: 0.0369 Accuracy: 95.35%\n",
            "Loss: 0.0357 Accuracy: 95.35%\n",
            "Loss: 0.0303 Accuracy: 95.35%\n",
            "Loss: 0.0165 Accuracy: 95.37%\n",
            "Loss: 0.0232 Accuracy: 95.38%\n",
            "Loss: 0.0378 Accuracy: 95.37%\n",
            "Loss: 0.0442 Accuracy: 95.36%\n",
            "Loss: 0.0306 Accuracy: 95.35%\n",
            "Loss: 0.0443 Accuracy: 95.32%\n",
            "Loss: 0.0198 Accuracy: 95.33%\n",
            "Loss: 0.0325 Accuracy: 95.32%\n",
            "Loss: 0.0323 Accuracy: 95.30%\n",
            "Loss: 0.0431 Accuracy: 95.28%\n",
            "Loss: 0.0263 Accuracy: 95.30%\n",
            "Loss: 0.0322 Accuracy: 95.31%\n",
            "Loss: 0.0285 Accuracy: 95.30%\n",
            "Loss: 0.0402 Accuracy: 95.30%\n",
            "Loss: 0.0341 Accuracy: 95.29%\n",
            "Loss: 0.0231 Accuracy: 95.31%\n",
            "Loss: 0.0289 Accuracy: 95.32%\n",
            "Loss: 0.0294 Accuracy: 95.33%\n",
            "Loss: 0.0471 Accuracy: 95.31%\n",
            "Loss: 0.0249 Accuracy: 95.33%\n",
            "Loss: 0.0291 Accuracy: 95.34%\n",
            "Loss: 0.0293 Accuracy: 95.33%\n",
            "Loss: 0.0419 Accuracy: 95.31%\n",
            "Loss: 0.0366 Accuracy: 95.30%\n",
            "Loss: 0.0308 Accuracy: 95.31%\n",
            "Loss: 0.0289 Accuracy: 95.30%\n",
            "Loss: 0.0357 Accuracy: 95.30%\n",
            "Loss: 0.0213 Accuracy: 95.32%\n",
            "Loss: 0.0307 Accuracy: 95.30%\n",
            "Loss: 0.0363 Accuracy: 95.29%\n",
            "Loss: 0.0206 Accuracy: 95.30%\n",
            "Loss: 0.0215 Accuracy: 95.30%\n",
            "Loss: 0.0323 Accuracy: 95.33%\n",
            "Loss: 0.0223 Accuracy: 95.34%\n",
            "Loss: 0.0418 Accuracy: 95.33%\n",
            "Loss: 0.0418 Accuracy: 95.30%\n",
            "Loss: 0.0367 Accuracy: 95.30%\n",
            "Loss: 0.0285 Accuracy: 95.30%\n",
            "Loss: 0.0329 Accuracy: 95.30%\n",
            "Loss: 0.0190 Accuracy: 95.31%\n",
            "Loss: 0.0301 Accuracy: 95.30%\n",
            "Loss: 0.0323 Accuracy: 95.31%\n",
            "Loss: 0.0290 Accuracy: 95.31%\n",
            "Loss: 0.0269 Accuracy: 95.32%\n",
            "Loss: 0.0337 Accuracy: 95.31%\n",
            "Loss: 0.0125 Accuracy: 95.33%\n",
            "Loss: 0.0378 Accuracy: 95.32%\n",
            "Loss: 0.0389 Accuracy: 95.30%\n",
            "Loss: 0.0312 Accuracy: 95.29%\n",
            "Loss: 0.0304 Accuracy: 95.28%\n",
            "Loss: 0.0236 Accuracy: 95.30%\n",
            "Loss: 0.0164 Accuracy: 95.32%\n",
            "Loss: 0.0301 Accuracy: 95.32%\n",
            "Loss: 0.0338 Accuracy: 95.33%\n",
            "Loss: 0.0202 Accuracy: 95.33%\n",
            "Loss: 0.0306 Accuracy: 95.34%\n",
            "Loss: 0.0374 Accuracy: 95.35%\n",
            "Loss: 0.0336 Accuracy: 95.33%\n",
            "Loss: 0.0299 Accuracy: 95.34%\n",
            "Loss: 0.0384 Accuracy: 95.32%\n",
            "Loss: 0.0246 Accuracy: 95.33%\n",
            "Loss: 0.0239 Accuracy: 95.34%\n",
            "Loss: 0.0249 Accuracy: 95.35%\n",
            "Loss: 0.0276 Accuracy: 95.35%\n",
            "Loss: 0.0479 Accuracy: 95.34%\n",
            "Loss: 0.0502 Accuracy: 95.31%\n",
            "Loss: 0.0189 Accuracy: 95.33%\n",
            "Loss: 0.0399 Accuracy: 95.32%\n",
            "Loss: 0.0358 Accuracy: 95.32%\n",
            "Loss: 0.0286 Accuracy: 95.33%\n",
            "Loss: 0.0250 Accuracy: 95.35%\n",
            "Loss: 0.0292 Accuracy: 95.34%\n",
            "Loss: 0.0217 Accuracy: 95.35%\n",
            "Loss: 0.0376 Accuracy: 95.35%\n",
            "Loss: 0.0230 Accuracy: 95.37%\n",
            "Loss: 0.0377 Accuracy: 95.36%\n",
            "Loss: 0.0216 Accuracy: 95.37%\n",
            "Loss: 0.0307 Accuracy: 95.37%\n",
            "Loss: 0.0277 Accuracy: 95.39%\n",
            "Loss: 0.0473 Accuracy: 95.38%\n",
            "Loss: 0.0226 Accuracy: 95.40%\n",
            "Loss: 0.0267 Accuracy: 95.40%\n",
            "Loss: 0.0327 Accuracy: 95.40%\n",
            "Loss: 0.0223 Accuracy: 95.41%\n",
            "Loss: 0.0284 Accuracy: 95.41%\n",
            "Loss: 0.0386 Accuracy: 95.40%\n",
            "Loss: 0.0376 Accuracy: 95.40%\n",
            "Loss: 0.0411 Accuracy: 95.40%\n",
            "Loss: 0.0251 Accuracy: 95.41%\n",
            "Loss: 0.0188 Accuracy: 95.42%\n",
            "Loss: 0.0448 Accuracy: 95.40%\n",
            "Loss: 0.0585 Accuracy: 95.38%\n",
            "Loss: 0.0395 Accuracy: 95.37%\n",
            "Loss: 0.0198 Accuracy: 95.39%\n",
            "Loss: 0.0313 Accuracy: 95.38%\n",
            "Loss: 0.0294 Accuracy: 95.38%\n",
            "Loss: 0.0390 Accuracy: 95.37%\n",
            "Loss: 0.0103 Accuracy: 95.39%\n",
            "Loss: 0.0200 Accuracy: 95.40%\n",
            "Loss: 0.0301 Accuracy: 95.40%\n",
            "Loss: 0.0218 Accuracy: 95.40%\n",
            "Loss: 0.0307 Accuracy: 95.41%\n",
            "Loss: 0.0357 Accuracy: 95.41%\n",
            "Loss: 0.0437 Accuracy: 95.40%\n",
            "Loss: 0.0293 Accuracy: 95.40%\n",
            "Loss: 0.0288 Accuracy: 95.39%\n",
            "Loss: 0.0476 Accuracy: 95.38%\n",
            "Loss: 0.0270 Accuracy: 95.38%\n",
            "Loss: 0.0554 Accuracy: 95.34%\n",
            "Loss: 0.0408 Accuracy: 95.34%\n",
            "Loss: 0.0270 Accuracy: 95.36%\n",
            "Loss: 0.0307 Accuracy: 95.37%\n",
            "Loss: 0.0239 Accuracy: 95.38%\n",
            "Loss: 0.0251 Accuracy: 95.38%\n",
            "Loss: 0.0366 Accuracy: 95.37%\n",
            "Loss: 0.0477 Accuracy: 95.36%\n",
            "Loss: 0.0548 Accuracy: 95.35%\n",
            "Loss: 0.0511 Accuracy: 95.33%\n",
            "Loss: 0.0203 Accuracy: 95.33%\n",
            "Loss: 0.0534 Accuracy: 95.32%\n",
            "Loss: 0.0368 Accuracy: 95.31%\n",
            "Loss: 0.0400 Accuracy: 95.30%\n",
            "Loss: 0.0270 Accuracy: 95.29%\n",
            "Loss: 0.0361 Accuracy: 95.29%\n",
            "Loss: 0.0282 Accuracy: 95.31%\n",
            "Loss: 0.0196 Accuracy: 95.31%\n",
            "Loss: 0.0288 Accuracy: 95.32%\n",
            "Loss: 0.0258 Accuracy: 95.33%\n",
            "Loss: 0.0254 Accuracy: 95.33%\n",
            "Loss: 0.0230 Accuracy: 95.34%\n",
            "Loss: 0.0205 Accuracy: 95.36%\n",
            "Loss: 0.0660 Accuracy: 95.34%\n",
            "Loss: 0.0305 Accuracy: 95.34%\n",
            "Loss: 0.0238 Accuracy: 95.36%\n",
            "Loss: 0.0284 Accuracy: 95.35%\n",
            "Loss: 0.0291 Accuracy: 95.34%\n",
            "Loss: 0.0355 Accuracy: 95.33%\n",
            "Loss: 0.0133 Accuracy: 95.34%\n",
            "Loss: 0.0405 Accuracy: 95.33%\n",
            "Loss: 0.0207 Accuracy: 95.34%\n",
            "Loss: 0.0346 Accuracy: 95.35%\n",
            "Loss: 0.0302 Accuracy: 95.35%\n",
            "Loss: 0.0354 Accuracy: 95.34%\n",
            "Loss: 0.0262 Accuracy: 95.35%\n",
            "Loss: 0.0211 Accuracy: 95.36%\n",
            "Loss: 0.0457 Accuracy: 95.35%\n",
            "Loss: 0.0309 Accuracy: 95.35%\n",
            "Loss: 0.0238 Accuracy: 95.36%\n",
            "Loss: 0.0387 Accuracy: 95.35%\n",
            "Loss: 0.0231 Accuracy: 95.36%\n",
            "Loss: 0.0479 Accuracy: 95.33%\n",
            "Loss: 0.0103 Accuracy: 95.34%\n",
            "Loss: 0.0295 Accuracy: 95.34%\n",
            "Loss: 0.0314 Accuracy: 95.34%\n",
            "Loss: 0.0574 Accuracy: 95.32%\n",
            "Loss: 0.0128 Accuracy: 95.33%\n",
            "Loss: 0.0467 Accuracy: 95.32%\n",
            "Loss: 0.0302 Accuracy: 95.32%\n",
            "Loss: 0.0194 Accuracy: 95.33%\n",
            "Loss: 0.0337 Accuracy: 95.32%\n",
            "Loss: 0.0381 Accuracy: 95.31%\n",
            "Loss: 0.0438 Accuracy: 95.30%\n",
            "Loss: 0.0452 Accuracy: 95.29%\n",
            "Loss: 0.0323 Accuracy: 95.30%\n",
            "Loss: 0.0489 Accuracy: 95.29%\n",
            "Loss: 0.0249 Accuracy: 95.30%\n",
            "Loss: 0.0262 Accuracy: 95.30%\n",
            "Loss: 0.0346 Accuracy: 95.29%\n",
            "Loss: 0.0284 Accuracy: 95.29%\n",
            "Loss: 0.0525 Accuracy: 95.27%\n",
            "Loss: 0.0204 Accuracy: 95.28%\n",
            "Loss: 0.0271 Accuracy: 95.28%\n",
            "Loss: 0.0358 Accuracy: 95.28%\n",
            "Loss: 0.0422 Accuracy: 95.27%\n",
            "Loss: 0.0435 Accuracy: 95.26%\n",
            "Loss: 0.0304 Accuracy: 95.24%\n",
            "Loss: 0.0223 Accuracy: 95.25%\n",
            "Loss: 0.0258 Accuracy: 95.26%\n",
            "Loss: 0.0212 Accuracy: 95.26%\n",
            "Loss: 0.0241 Accuracy: 95.26%\n",
            "Loss: 0.0324 Accuracy: 95.27%\n",
            "Loss: 0.0283 Accuracy: 95.28%\n",
            "Loss: 0.0291 Accuracy: 95.28%\n",
            "Loss: 0.0198 Accuracy: 95.29%\n",
            "Loss: 0.0440 Accuracy: 95.28%\n",
            "Loss: 0.0404 Accuracy: 95.27%\n",
            "Loss: 0.0407 Accuracy: 95.25%\n",
            "Loss: 0.0489 Accuracy: 95.24%\n",
            "Loss: 0.0443 Accuracy: 95.22%\n",
            "Loss: 0.0288 Accuracy: 95.23%\n",
            "Loss: 0.0353 Accuracy: 95.22%\n",
            "Loss: 0.0290 Accuracy: 95.23%\n",
            "Loss: 0.0484 Accuracy: 95.22%\n",
            "Loss: 0.0411 Accuracy: 95.22%\n",
            "Loss: 0.0239 Accuracy: 95.22%\n",
            "Loss: 0.0383 Accuracy: 95.22%\n",
            "Loss: 0.0356 Accuracy: 95.22%\n",
            "Loss: 0.0414 Accuracy: 95.22%\n",
            "Loss: 0.0246 Accuracy: 95.22%\n",
            "Loss: 0.0241 Accuracy: 95.22%\n",
            "Loss: 0.0340 Accuracy: 95.22%\n",
            "Loss: 0.0271 Accuracy: 95.22%\n",
            "Loss: 0.0312 Accuracy: 95.22%\n",
            "Loss: 0.0295 Accuracy: 95.22%\n",
            "Loss: 0.0346 Accuracy: 95.22%\n",
            "Loss: 0.0319 Accuracy: 95.21%\n",
            "Loss: 0.0201 Accuracy: 95.22%\n",
            "Loss: 0.0376 Accuracy: 95.21%\n",
            "Loss: 0.0330 Accuracy: 95.22%\n",
            "Loss: 0.0388 Accuracy: 95.21%\n",
            "Loss: 0.0342 Accuracy: 95.21%\n",
            "Loss: 0.0339 Accuracy: 95.21%\n",
            "Loss: 0.0286 Accuracy: 95.20%\n",
            "Loss: 0.0406 Accuracy: 95.20%\n",
            "Loss: 0.0318 Accuracy: 95.21%\n",
            "Loss: 0.0166 Accuracy: 95.22%\n",
            "Loss: 0.0226 Accuracy: 95.22%\n",
            "Loss: 0.0213 Accuracy: 95.23%\n",
            "Loss: 0.0301 Accuracy: 95.23%\n",
            "Loss: 0.0461 Accuracy: 95.22%\n",
            "Loss: 0.0231 Accuracy: 95.22%\n",
            "Loss: 0.0282 Accuracy: 95.22%\n",
            "Loss: 0.0286 Accuracy: 95.22%\n",
            "Loss: 0.0485 Accuracy: 95.21%\n",
            "Loss: 0.0356 Accuracy: 95.20%\n",
            "Loss: 0.0367 Accuracy: 95.20%\n",
            "Loss: 0.0238 Accuracy: 95.20%\n",
            "Loss: 0.0345 Accuracy: 95.20%\n",
            "Loss: 0.0320 Accuracy: 95.20%\n",
            "Loss: 0.0455 Accuracy: 95.19%\n",
            "Loss: 0.0290 Accuracy: 95.20%\n",
            "Loss: 0.0317 Accuracy: 95.20%\n",
            "Loss: 0.0303 Accuracy: 95.21%\n",
            "Loss: 0.0298 Accuracy: 95.21%\n",
            "Loss: 0.0322 Accuracy: 95.21%\n",
            "Loss: 0.0422 Accuracy: 95.20%\n",
            "Loss: 0.0261 Accuracy: 95.21%\n",
            "Loss: 0.0245 Accuracy: 95.20%\n",
            "Loss: 0.0520 Accuracy: 95.18%\n",
            "Loss: 0.0286 Accuracy: 95.19%\n",
            "Loss: 0.0439 Accuracy: 95.19%\n",
            "Loss: 0.0304 Accuracy: 95.19%\n",
            "Loss: 0.0303 Accuracy: 95.19%\n",
            "Loss: 0.0302 Accuracy: 95.19%\n",
            "Loss: 0.0317 Accuracy: 95.19%\n",
            "Loss: 0.0314 Accuracy: 95.19%\n",
            "Loss: 0.0478 Accuracy: 95.19%\n",
            "Loss: 0.0425 Accuracy: 95.19%\n",
            "Loss: 0.0427 Accuracy: 95.18%\n",
            "Loss: 0.0246 Accuracy: 95.19%\n",
            "Loss: 0.0361 Accuracy: 95.18%\n",
            "Loss: 0.0191 Accuracy: 95.19%\n",
            "Loss: 0.0321 Accuracy: 95.19%\n",
            "Loss: 0.0328 Accuracy: 95.19%\n",
            "Loss: 0.0382 Accuracy: 95.19%\n",
            "Loss: 0.0365 Accuracy: 95.17%\n",
            "Loss: 0.0359 Accuracy: 95.17%\n",
            "Loss: 0.0361 Accuracy: 95.17%\n",
            "Loss: 0.0331 Accuracy: 95.18%\n",
            "Loss: 0.0266 Accuracy: 95.18%\n",
            "Loss: 0.0287 Accuracy: 95.18%\n",
            "Loss: 0.0245 Accuracy: 95.19%\n",
            "Loss: 0.0332 Accuracy: 95.18%\n",
            "Loss: 0.0299 Accuracy: 95.18%\n",
            "Loss: 0.0282 Accuracy: 95.18%\n",
            "Loss: 0.0243 Accuracy: 95.19%\n",
            "Loss: 0.0252 Accuracy: 95.20%\n",
            "Loss: 0.0392 Accuracy: 95.19%\n",
            "Loss: 0.0286 Accuracy: 95.19%\n",
            "Loss: 0.0194 Accuracy: 95.19%\n",
            "Loss: 0.0278 Accuracy: 95.19%\n",
            "Loss: 0.0473 Accuracy: 95.18%\n",
            "Loss: 0.0623 Accuracy: 95.16%\n",
            "Loss: 0.0259 Accuracy: 95.17%\n",
            "Loss: 0.0286 Accuracy: 95.16%\n",
            "Loss: 0.0364 Accuracy: 95.16%\n",
            "Loss: 0.0230 Accuracy: 95.17%\n",
            "Loss: 0.0437 Accuracy: 95.17%\n",
            "Loss: 0.0239 Accuracy: 95.17%\n",
            "Loss: 0.0274 Accuracy: 95.17%\n",
            "Loss: 0.0257 Accuracy: 95.18%\n",
            "Loss: 0.0317 Accuracy: 95.18%\n",
            "Loss: 0.0301 Accuracy: 95.18%\n",
            "Loss: 0.0207 Accuracy: 95.19%\n",
            "Loss: 0.0418 Accuracy: 95.18%\n",
            "Loss: 0.0345 Accuracy: 95.18%\n",
            "Loss: 0.0238 Accuracy: 95.18%\n",
            "Loss: 0.0332 Accuracy: 95.18%\n",
            "Loss: 0.0230 Accuracy: 95.18%\n",
            "Loss: 0.0308 Accuracy: 95.19%\n",
            "Loss: 0.0372 Accuracy: 95.18%\n",
            "Loss: 0.0179 Accuracy: 95.19%\n",
            "Loss: 0.0334 Accuracy: 95.18%\n",
            "Loss: 0.0176 Accuracy: 95.19%\n",
            "Loss: 0.0311 Accuracy: 95.19%\n",
            "Loss: 0.0183 Accuracy: 95.19%\n",
            "Loss: 0.0155 Accuracy: 95.20%\n",
            "Loss: 0.0410 Accuracy: 95.20%\n",
            "Loss: 0.0253 Accuracy: 95.20%\n",
            "Loss: 0.0326 Accuracy: 95.20%\n",
            "Loss: 0.0293 Accuracy: 95.20%\n",
            "Loss: 0.0193 Accuracy: 95.21%\n",
            "Loss: 0.0220 Accuracy: 95.21%\n",
            "Loss: 0.0352 Accuracy: 95.20%\n",
            "Loss: 0.0388 Accuracy: 95.19%\n",
            "Loss: 0.0223 Accuracy: 95.20%\n",
            "Loss: 0.0395 Accuracy: 95.20%\n",
            "Loss: 0.0319 Accuracy: 95.19%\n",
            "Loss: 0.0272 Accuracy: 95.19%\n",
            "Loss: 0.0298 Accuracy: 95.19%\n",
            "Loss: 0.0375 Accuracy: 95.19%\n",
            "Loss: 0.0291 Accuracy: 95.19%\n",
            "Loss: 0.0268 Accuracy: 95.19%\n",
            "Loss: 0.0308 Accuracy: 95.19%\n",
            "Loss: 0.0297 Accuracy: 95.20%\n",
            "Loss: 0.0185 Accuracy: 95.20%\n",
            "Loss: 0.0269 Accuracy: 95.20%\n",
            "Loss: 0.0352 Accuracy: 95.20%\n",
            "Loss: 0.0392 Accuracy: 95.19%\n",
            "Loss: 0.0207 Accuracy: 95.20%\n",
            "Loss: 0.0209 Accuracy: 95.20%\n",
            "Loss: 0.0592 Accuracy: 95.18%\n",
            "Loss: 0.0286 Accuracy: 95.18%\n",
            "Loss: 0.0272 Accuracy: 95.18%\n",
            "Loss: 0.0280 Accuracy: 95.19%\n",
            "Loss: 0.0307 Accuracy: 95.19%\n",
            "Loss: 0.0392 Accuracy: 95.18%\n",
            "Loss: 0.0280 Accuracy: 95.18%\n",
            "Loss: 0.0249 Accuracy: 95.19%\n",
            "Loss: 0.0413 Accuracy: 95.19%\n",
            "Loss: 0.0200 Accuracy: 95.19%\n",
            "Loss: 0.0412 Accuracy: 95.19%\n",
            "Loss: 0.0259 Accuracy: 95.19%\n",
            "Loss: 0.0174 Accuracy: 95.20%\n",
            "Loss: 0.0352 Accuracy: 95.19%\n",
            "Loss: 0.0252 Accuracy: 95.19%\n",
            "Loss: 0.0414 Accuracy: 95.18%\n",
            "Loss: 0.0350 Accuracy: 95.18%\n",
            "Loss: 0.0195 Accuracy: 95.18%\n",
            "Loss: 0.0339 Accuracy: 95.18%\n",
            "Loss: 0.0427 Accuracy: 95.18%\n",
            "Loss: 0.0277 Accuracy: 95.18%\n",
            "Loss: 0.0231 Accuracy: 95.19%\n",
            "Loss: 0.0366 Accuracy: 95.19%\n",
            "Loss: 0.0370 Accuracy: 95.18%\n",
            "Loss: 0.0390 Accuracy: 95.17%\n",
            "Loss: 0.0373 Accuracy: 95.17%\n",
            "Loss: 0.0320 Accuracy: 95.17%\n",
            "Loss: 0.0292 Accuracy: 95.18%\n",
            "Loss: 0.0187 Accuracy: 95.18%\n",
            "Loss: 0.0345 Accuracy: 95.18%\n",
            "Loss: 0.0335 Accuracy: 95.18%\n",
            "Loss: 0.0329 Accuracy: 95.18%\n",
            "Loss: 0.0301 Accuracy: 95.18%\n",
            "Loss: 0.0418 Accuracy: 95.17%\n",
            "Loss: 0.0271 Accuracy: 95.18%\n",
            "Loss: 0.0251 Accuracy: 95.18%\n",
            "Loss: 0.0305 Accuracy: 95.19%\n",
            "Loss: 0.0290 Accuracy: 95.19%\n",
            "Loss: 0.0432 Accuracy: 95.19%\n",
            "Loss: 0.0267 Accuracy: 95.18%\n",
            "Loss: 0.0416 Accuracy: 95.17%\n",
            "Loss: 0.0417 Accuracy: 95.17%\n",
            "Loss: 0.0434 Accuracy: 95.17%\n",
            "Loss: 0.0250 Accuracy: 95.17%\n",
            "Loss: 0.0417 Accuracy: 95.16%\n",
            "Loss: 0.0308 Accuracy: 95.17%\n",
            "Loss: 0.0266 Accuracy: 95.17%\n",
            "Loss: 0.0223 Accuracy: 95.18%\n",
            "Loss: 0.0279 Accuracy: 95.18%\n",
            "Loss: 0.0347 Accuracy: 95.18%\n",
            "Loss: 0.0251 Accuracy: 95.18%\n",
            "Loss: 0.0496 Accuracy: 95.17%\n",
            "Loss: 0.0337 Accuracy: 95.17%\n",
            "Loss: 0.0315 Accuracy: 95.17%\n",
            "Loss: 0.0344 Accuracy: 95.16%\n",
            "Loss: 0.0388 Accuracy: 95.16%\n",
            "Loss: 0.0218 Accuracy: 95.16%\n",
            "Loss: 0.0413 Accuracy: 95.16%\n",
            "Loss: 0.0211 Accuracy: 95.16%\n",
            "Loss: 0.0311 Accuracy: 95.16%\n",
            "Loss: 0.0219 Accuracy: 95.16%\n",
            "Loss: 0.0591 Accuracy: 95.15%\n",
            "Loss: 0.0380 Accuracy: 95.15%\n",
            "Loss: 0.0346 Accuracy: 95.15%\n",
            "Loss: 0.0317 Accuracy: 95.15%\n",
            "Loss: 0.0313 Accuracy: 95.15%\n",
            "Loss: 0.0264 Accuracy: 95.16%\n",
            "Loss: 0.0388 Accuracy: 95.15%\n",
            "Loss: 0.0407 Accuracy: 95.14%\n",
            "Loss: 0.0373 Accuracy: 95.14%\n",
            "Loss: 0.0394 Accuracy: 95.13%\n",
            "Loss: 0.0278 Accuracy: 95.13%\n",
            "Loss: 0.0322 Accuracy: 95.13%\n",
            "Loss: 0.0173 Accuracy: 95.13%\n",
            "Loss: 0.0310 Accuracy: 95.13%\n",
            "Loss: 0.0349 Accuracy: 95.13%\n",
            "Loss: 0.0328 Accuracy: 95.13%\n",
            "Loss: 0.0329 Accuracy: 95.13%\n",
            "Loss: 0.0302 Accuracy: 95.13%\n",
            "Loss: 0.0281 Accuracy: 95.13%\n",
            "Loss: 0.0458 Accuracy: 95.13%\n",
            "Loss: 0.0254 Accuracy: 95.13%\n",
            "Loss: 0.0197 Accuracy: 95.14%\n",
            "Loss: 0.0333 Accuracy: 95.13%\n",
            "Loss: 0.0419 Accuracy: 95.13%\n",
            "Loss: 0.0262 Accuracy: 95.13%\n",
            "Loss: 0.0294 Accuracy: 95.13%\n",
            "Loss: 0.0304 Accuracy: 95.13%\n",
            "Loss: 0.0315 Accuracy: 95.13%\n",
            "Loss: 0.0423 Accuracy: 95.12%\n",
            "Loss: 0.0477 Accuracy: 95.12%\n",
            "Loss: 0.0206 Accuracy: 95.12%\n",
            "Loss: 0.0424 Accuracy: 95.12%\n",
            "Loss: 0.0319 Accuracy: 95.12%\n",
            "Loss: 0.0492 Accuracy: 95.11%\n",
            "Loss: 0.0434 Accuracy: 95.11%\n",
            "Loss: 0.0167 Accuracy: 95.11%\n",
            "Loss: 0.0308 Accuracy: 95.11%\n",
            "Loss: 0.0224 Accuracy: 95.12%\n",
            "Loss: 0.0322 Accuracy: 95.12%\n",
            "Loss: 0.0214 Accuracy: 95.13%\n",
            "Loss: 0.0426 Accuracy: 95.13%\n",
            "Loss: 0.0302 Accuracy: 95.13%\n",
            "Loss: 0.0176 Accuracy: 95.13%\n",
            "Loss: 0.0376 Accuracy: 95.13%\n",
            "Loss: 0.0352 Accuracy: 95.12%\n",
            "Loss: 0.0159 Accuracy: 95.13%\n",
            "Loss: 0.0298 Accuracy: 95.13%\n",
            "Loss: 0.0450 Accuracy: 95.13%\n",
            "Loss: 0.0297 Accuracy: 95.12%\n",
            "Loss: 0.0341 Accuracy: 95.13%\n",
            "Loss: 0.0277 Accuracy: 95.13%\n",
            "Loss: 0.0327 Accuracy: 95.13%\n",
            "Loss: 0.0323 Accuracy: 95.13%\n",
            "Loss: 0.0421 Accuracy: 95.13%\n",
            "Loss: 0.0340 Accuracy: 95.12%\n",
            "Loss: 0.0296 Accuracy: 95.12%\n",
            "Loss: 0.0515 Accuracy: 95.11%\n",
            "Loss: 0.0395 Accuracy: 95.11%\n",
            "Loss: 0.0327 Accuracy: 95.11%\n",
            "Loss: 0.0380 Accuracy: 95.11%\n",
            "Loss: 0.0327 Accuracy: 95.11%\n",
            "Loss: 0.0278 Accuracy: 95.11%\n",
            "Loss: 0.0293 Accuracy: 95.11%\n",
            "Loss: 0.0306 Accuracy: 95.11%\n",
            "Loss: 0.0376 Accuracy: 95.11%\n",
            "Loss: 0.0417 Accuracy: 95.10%\n",
            "Loss: 0.0383 Accuracy: 95.10%\n",
            "Loss: 0.0366 Accuracy: 95.10%\n",
            "Loss: 0.0402 Accuracy: 95.10%\n",
            "Loss: 0.0300 Accuracy: 95.10%\n",
            "Loss: 0.0388 Accuracy: 95.09%\n",
            "Loss: 0.0332 Accuracy: 95.09%\n",
            "Loss: 0.0367 Accuracy: 95.09%\n",
            "Loss: 0.0265 Accuracy: 95.10%\n",
            "Loss: 0.0266 Accuracy: 95.10%\n",
            "Loss: 0.0392 Accuracy: 95.10%\n",
            "Loss: 0.0331 Accuracy: 95.10%\n",
            "Loss: 0.0342 Accuracy: 95.10%\n",
            "Loss: 0.0477 Accuracy: 95.09%\n",
            "Loss: 0.0264 Accuracy: 95.10%\n",
            "Loss: 0.0210 Accuracy: 95.10%\n",
            "Loss: 0.0415 Accuracy: 95.10%\n",
            "Loss: 0.0252 Accuracy: 95.10%\n",
            "Loss: 0.0280 Accuracy: 95.11%\n",
            "Loss: 0.0288 Accuracy: 95.11%\n",
            "Loss: 0.0323 Accuracy: 95.10%\n",
            "Loss: 0.0158 Accuracy: 95.11%\n",
            "Loss: 0.0132 Accuracy: 95.11%\n",
            "Loss: 0.0370 Accuracy: 95.11%\n",
            "Loss: 0.0270 Accuracy: 95.12%\n",
            "Loss: 0.0227 Accuracy: 95.12%\n",
            "Loss: 0.0423 Accuracy: 95.12%\n",
            "Loss: 0.0392 Accuracy: 95.12%\n",
            "Loss: 0.0312 Accuracy: 95.11%\n",
            "Loss: 0.0184 Accuracy: 95.12%\n",
            "Loss: 0.0389 Accuracy: 95.11%\n",
            "Loss: 0.0446 Accuracy: 95.10%\n",
            "Loss: 0.0288 Accuracy: 95.10%\n",
            "Loss: 0.0438 Accuracy: 95.10%\n",
            "Loss: 0.0487 Accuracy: 95.09%\n",
            "Loss: 0.0246 Accuracy: 95.09%\n",
            "Loss: 0.0221 Accuracy: 95.09%\n",
            "Loss: 0.0375 Accuracy: 95.09%\n",
            "Loss: 0.0340 Accuracy: 95.09%\n",
            "Loss: 0.0339 Accuracy: 95.09%\n",
            "Loss: 0.0421 Accuracy: 95.08%\n",
            "Loss: 0.0272 Accuracy: 95.08%\n",
            "Loss: 0.0299 Accuracy: 95.08%\n",
            "Loss: 0.0441 Accuracy: 95.08%\n",
            "Loss: 0.0301 Accuracy: 95.08%\n",
            "Loss: 0.0300 Accuracy: 95.08%\n",
            "Loss: 0.0353 Accuracy: 95.08%\n",
            "Loss: 0.0254 Accuracy: 95.08%\n",
            "Loss: 0.0360 Accuracy: 95.08%\n",
            "Loss: 0.0247 Accuracy: 95.09%\n",
            "Loss: 0.0111 Accuracy: 95.10%\n",
            "Loss: 0.0182 Accuracy: 95.10%\n",
            "Loss: 0.0319 Accuracy: 95.10%\n",
            "Loss: 0.0322 Accuracy: 95.10%\n",
            "Loss: 0.0315 Accuracy: 95.10%\n",
            "Loss: 0.0220 Accuracy: 95.11%\n",
            "Loss: 0.0203 Accuracy: 95.11%\n",
            "Loss: 0.0444 Accuracy: 95.11%\n",
            "Loss: 0.0266 Accuracy: 95.11%\n",
            "Loss: 0.0289 Accuracy: 95.11%\n",
            "Loss: 0.0303 Accuracy: 95.10%\n",
            "Loss: 0.0105 Accuracy: 95.11%\n",
            "Loss: 0.0307 Accuracy: 95.12%\n",
            "Loss: 0.0292 Accuracy: 95.11%\n",
            "Loss: 0.0449 Accuracy: 95.11%\n",
            "Loss: 0.0449 Accuracy: 95.10%\n",
            "Loss: 0.0295 Accuracy: 95.11%\n",
            "Loss: 0.0420 Accuracy: 95.10%\n",
            "Loss: 0.0397 Accuracy: 95.10%\n",
            "Loss: 0.0349 Accuracy: 95.10%\n",
            "Loss: 0.0396 Accuracy: 95.09%\n",
            "Loss: 0.0211 Accuracy: 95.09%\n",
            "Loss: 0.0206 Accuracy: 95.10%\n",
            "Loss: 0.0255 Accuracy: 95.10%\n",
            "Loss: 0.0347 Accuracy: 95.10%\n",
            "Loss: 0.0335 Accuracy: 95.10%\n",
            "Loss: 0.0369 Accuracy: 95.10%\n",
            "Loss: 0.0287 Accuracy: 95.10%\n",
            "Loss: 0.0211 Accuracy: 95.10%\n",
            "Loss: 0.0302 Accuracy: 95.10%\n",
            "Loss: 0.0544 Accuracy: 95.10%\n",
            "Loss: 0.0374 Accuracy: 95.09%\n",
            "Loss: 0.0373 Accuracy: 95.09%\n",
            "Loss: 0.0346 Accuracy: 95.09%\n",
            "Loss: 0.0288 Accuracy: 95.09%\n",
            "Loss: 0.0464 Accuracy: 95.08%\n",
            "Loss: 0.0372 Accuracy: 95.08%\n",
            "Loss: 0.0250 Accuracy: 95.09%\n",
            "Loss: 0.0341 Accuracy: 95.09%\n",
            "Loss: 0.0227 Accuracy: 95.09%\n",
            "Loss: 0.0313 Accuracy: 95.09%\n",
            "Loss: 0.0488 Accuracy: 95.09%\n",
            "Loss: 0.0370 Accuracy: 95.09%\n",
            "Loss: 0.0258 Accuracy: 95.09%\n",
            "Loss: 0.0452 Accuracy: 95.09%\n",
            "Loss: 0.0198 Accuracy: 95.09%\n",
            "Loss: 0.0254 Accuracy: 95.09%\n",
            "Loss: 0.0237 Accuracy: 95.10%\n",
            "Loss: 0.0204 Accuracy: 95.10%\n",
            "Loss: 0.0276 Accuracy: 95.11%\n",
            "Loss: 0.0370 Accuracy: 95.11%\n",
            "Loss: 0.0285 Accuracy: 95.11%\n",
            "Loss: 0.0348 Accuracy: 95.10%\n",
            "Loss: 0.0304 Accuracy: 95.10%\n",
            "Loss: 0.0395 Accuracy: 95.10%\n",
            "Loss: 0.0310 Accuracy: 95.09%\n",
            "Loss: 0.0301 Accuracy: 95.10%\n",
            "Loss: 0.0266 Accuracy: 95.10%\n",
            "Loss: 0.0434 Accuracy: 95.10%\n",
            "Loss: 0.0284 Accuracy: 95.10%\n",
            "Loss: 0.0203 Accuracy: 95.10%\n",
            "Loss: 0.0160 Accuracy: 95.11%\n",
            "Loss: 0.0345 Accuracy: 95.11%\n",
            "Loss: 0.0353 Accuracy: 95.11%\n",
            "Loss: 0.0264 Accuracy: 95.11%\n",
            "Loss: 0.0330 Accuracy: 95.11%\n",
            "Loss: 0.0532 Accuracy: 95.10%\n",
            "Loss: 0.0298 Accuracy: 95.11%\n",
            "Loss: 0.0234 Accuracy: 95.11%\n",
            "Loss: 0.0232 Accuracy: 95.11%\n",
            "Loss: 0.0458 Accuracy: 95.10%\n",
            "Loss: 0.0371 Accuracy: 95.10%\n",
            "Loss: 0.0325 Accuracy: 95.10%\n",
            "Loss: 0.0324 Accuracy: 95.10%\n",
            "Loss: 0.0258 Accuracy: 95.10%\n",
            "Loss: 0.0135 Accuracy: 95.10%\n",
            "Loss: 0.0332 Accuracy: 95.10%\n",
            "Loss: 0.0361 Accuracy: 95.10%\n",
            "Loss: 0.0423 Accuracy: 95.10%\n",
            "Loss: 0.0475 Accuracy: 95.10%\n",
            "Loss: 0.0436 Accuracy: 95.09%\n",
            "Loss: 0.0296 Accuracy: 95.10%\n",
            "Loss: 0.0148 Accuracy: 95.10%\n",
            "Loss: 0.0354 Accuracy: 95.10%\n",
            "Loss: 0.0184 Accuracy: 95.10%\n",
            "Loss: 0.0532 Accuracy: 95.09%\n",
            "Loss: 0.0304 Accuracy: 95.09%\n",
            "Loss: 0.0362 Accuracy: 95.09%\n",
            "Loss: 0.0325 Accuracy: 95.09%\n",
            "Loss: 0.0381 Accuracy: 95.09%\n",
            "Loss: 0.0337 Accuracy: 95.09%\n",
            "Loss: 0.0362 Accuracy: 95.09%\n",
            "Loss: 0.0236 Accuracy: 95.09%\n",
            "Loss: 0.0236 Accuracy: 95.10%\n",
            "Loss: 0.0300 Accuracy: 95.10%\n",
            "Loss: 0.0271 Accuracy: 95.10%\n",
            "Loss: 0.0440 Accuracy: 95.10%\n",
            "Loss: 0.0223 Accuracy: 95.10%\n",
            "Loss: 0.0163 Accuracy: 95.11%\n",
            "Loss: 0.0406 Accuracy: 95.10%\n",
            "Loss: 0.0360 Accuracy: 95.10%\n",
            "Loss: 0.0493 Accuracy: 95.09%\n",
            "Loss: 0.0503 Accuracy: 95.09%\n",
            "Loss: 0.0352 Accuracy: 95.09%\n",
            "Loss: 0.0296 Accuracy: 95.09%\n",
            "Loss: 0.0297 Accuracy: 95.09%\n",
            "Loss: 0.0428 Accuracy: 95.08%\n",
            "Loss: 0.0234 Accuracy: 95.08%\n",
            "Loss: 0.0415 Accuracy: 95.08%\n",
            "Loss: 0.0205 Accuracy: 95.08%\n",
            "Loss: 0.0344 Accuracy: 95.08%\n",
            "Loss: 0.0280 Accuracy: 95.08%\n",
            "Loss: 0.0194 Accuracy: 95.09%\n",
            "Loss: 0.0319 Accuracy: 95.09%\n",
            "Loss: 0.0292 Accuracy: 95.09%\n",
            "Loss: 0.0209 Accuracy: 95.09%\n",
            "Loss: 0.0418 Accuracy: 95.09%\n",
            "Loss: 0.0291 Accuracy: 95.09%\n",
            "Loss: 0.0396 Accuracy: 95.08%\n",
            "Loss: 0.0315 Accuracy: 95.08%\n",
            "Loss: 0.0263 Accuracy: 95.08%\n",
            "Loss: 0.0281 Accuracy: 95.08%\n",
            "Loss: 0.0330 Accuracy: 95.08%\n",
            "Loss: 0.0364 Accuracy: 95.08%\n",
            "Loss: 0.0143 Accuracy: 95.08%\n",
            "Final Test Accuracy: 95.08%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "batch_size = 64\n",
        "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        _, prediction = torch.max(outputs.data, 1)\n",
        "        new_labels = [[0.0 for tt in range(10)] for t in range(len(labels))]\n",
        "        for t in range(len(labels)):\n",
        "          new_labels[t][labels[t]] = 1.0\n",
        "        new_labels = torch.tensor(new_labels)\n",
        "        new_labels = new_labels.to(device)\n",
        "        labels = labels.to(device)\n",
        "        prediction = prediction.to(device)\n",
        "        loss = criterion(outputs, new_labels)\n",
        "        total += (prediction == prediction).sum().item()\n",
        "        correct += (prediction == labels).sum().item()\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy: {(correct / total * 100):.2f}%') # Loss, Accuracy\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%') # Accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYZqCAsHXnNf",
        "outputId": "6546b904-63f2-46e2-890e-d5777cb74eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Loss: 0.1725 Accuracy: 73.44%\n",
            "Loss: 0.2176 Accuracy: 71.88%\n",
            "Loss: 0.1776 Accuracy: 72.92%\n",
            "Loss: 0.1896 Accuracy: 73.44%\n",
            "Loss: 0.1818 Accuracy: 73.12%\n",
            "Loss: 0.1167 Accuracy: 73.44%\n",
            "Loss: 0.2132 Accuracy: 72.32%\n",
            "Loss: 0.2505 Accuracy: 72.07%\n",
            "Loss: 0.2348 Accuracy: 72.05%\n",
            "Loss: 0.0982 Accuracy: 73.28%\n",
            "Loss: 0.2489 Accuracy: 73.01%\n",
            "Loss: 0.1576 Accuracy: 73.18%\n",
            "Loss: 0.1467 Accuracy: 73.08%\n",
            "Loss: 0.1901 Accuracy: 72.99%\n",
            "Loss: 0.1669 Accuracy: 73.23%\n",
            "Loss: 0.1958 Accuracy: 72.75%\n",
            "Loss: 0.2499 Accuracy: 72.43%\n",
            "Loss: 0.1776 Accuracy: 71.79%\n",
            "Loss: 0.2677 Accuracy: 71.79%\n",
            "Loss: 0.1864 Accuracy: 71.88%\n",
            "Loss: 0.1347 Accuracy: 72.25%\n",
            "Loss: 0.2490 Accuracy: 72.16%\n",
            "Loss: 0.2556 Accuracy: 72.15%\n",
            "Loss: 0.2392 Accuracy: 71.88%\n",
            "Loss: 0.1305 Accuracy: 71.94%\n",
            "Loss: 0.1455 Accuracy: 72.36%\n",
            "Loss: 0.2197 Accuracy: 72.34%\n",
            "Loss: 0.2406 Accuracy: 72.43%\n",
            "Loss: 0.0925 Accuracy: 72.90%\n",
            "Loss: 0.2066 Accuracy: 72.97%\n",
            "Loss: 0.3833 Accuracy: 72.38%\n",
            "Loss: 0.1535 Accuracy: 72.56%\n",
            "Loss: 0.2030 Accuracy: 72.73%\n",
            "Loss: 0.1938 Accuracy: 72.66%\n",
            "Loss: 0.1768 Accuracy: 72.77%\n",
            "Loss: 0.2235 Accuracy: 72.87%\n",
            "Loss: 0.1437 Accuracy: 72.93%\n",
            "Loss: 0.2465 Accuracy: 72.86%\n",
            "Loss: 0.2016 Accuracy: 72.96%\n",
            "Loss: 0.1392 Accuracy: 73.24%\n",
            "Loss: 0.2134 Accuracy: 73.32%\n",
            "Loss: 0.1926 Accuracy: 73.33%\n",
            "Loss: 0.1087 Accuracy: 73.55%\n",
            "Loss: 0.1589 Accuracy: 73.54%\n",
            "Loss: 0.1288 Accuracy: 73.72%\n",
            "Loss: 0.2074 Accuracy: 73.71%\n",
            "Loss: 0.2062 Accuracy: 73.54%\n",
            "Loss: 0.2164 Accuracy: 73.47%\n",
            "Loss: 0.2657 Accuracy: 73.31%\n",
            "Loss: 0.1748 Accuracy: 73.38%\n",
            "Loss: 0.1816 Accuracy: 73.28%\n",
            "Loss: 0.1748 Accuracy: 73.26%\n",
            "Loss: 0.2067 Accuracy: 73.08%\n",
            "Loss: 0.2760 Accuracy: 73.00%\n",
            "Loss: 0.1957 Accuracy: 73.01%\n",
            "Loss: 0.2941 Accuracy: 72.96%\n",
            "Loss: 0.1642 Accuracy: 73.08%\n",
            "Loss: 0.1160 Accuracy: 73.30%\n",
            "Loss: 0.1105 Accuracy: 73.36%\n",
            "Loss: 0.2349 Accuracy: 73.31%\n",
            "Loss: 0.1727 Accuracy: 73.39%\n",
            "Loss: 0.1006 Accuracy: 73.49%\n",
            "Loss: 0.1706 Accuracy: 73.51%\n",
            "Loss: 0.1954 Accuracy: 73.46%\n",
            "Loss: 0.2753 Accuracy: 73.39%\n",
            "Loss: 0.1699 Accuracy: 73.37%\n",
            "Loss: 0.2094 Accuracy: 73.37%\n",
            "Loss: 0.1640 Accuracy: 73.41%\n",
            "Loss: 0.2081 Accuracy: 73.41%\n",
            "Loss: 0.2281 Accuracy: 73.33%\n",
            "Loss: 0.1754 Accuracy: 73.35%\n",
            "Loss: 0.1682 Accuracy: 73.39%\n",
            "Loss: 0.2002 Accuracy: 73.35%\n",
            "Loss: 0.1188 Accuracy: 73.52%\n",
            "Loss: 0.2032 Accuracy: 73.50%\n",
            "Loss: 0.2383 Accuracy: 73.52%\n",
            "Loss: 0.1213 Accuracy: 73.64%\n",
            "Loss: 0.1760 Accuracy: 73.72%\n",
            "Loss: 0.1703 Accuracy: 73.67%\n",
            "Loss: 0.1851 Accuracy: 73.65%\n",
            "Loss: 0.1816 Accuracy: 73.65%\n",
            "Loss: 0.2084 Accuracy: 73.61%\n",
            "Loss: 0.2548 Accuracy: 73.38%\n",
            "Loss: 0.2023 Accuracy: 73.31%\n",
            "Loss: 0.1740 Accuracy: 73.31%\n",
            "Loss: 0.2069 Accuracy: 73.26%\n",
            "Loss: 0.1896 Accuracy: 73.29%\n",
            "Loss: 0.2933 Accuracy: 73.17%\n",
            "Loss: 0.1875 Accuracy: 73.16%\n",
            "Loss: 0.1860 Accuracy: 73.09%\n",
            "Loss: 0.2221 Accuracy: 73.01%\n",
            "Loss: 0.1740 Accuracy: 73.05%\n",
            "Loss: 0.2717 Accuracy: 73.02%\n",
            "Loss: 0.2069 Accuracy: 73.04%\n",
            "Loss: 0.2039 Accuracy: 73.01%\n",
            "Loss: 0.1738 Accuracy: 73.05%\n",
            "Loss: 0.2336 Accuracy: 73.07%\n",
            "Loss: 0.1918 Accuracy: 73.05%\n",
            "Loss: 0.1171 Accuracy: 73.14%\n",
            "Loss: 0.1840 Accuracy: 73.14%\n",
            "Loss: 0.2371 Accuracy: 73.13%\n",
            "Loss: 0.2381 Accuracy: 73.07%\n",
            "Loss: 0.2093 Accuracy: 73.06%\n",
            "Loss: 0.2347 Accuracy: 72.97%\n",
            "Loss: 0.2066 Accuracy: 72.98%\n",
            "Loss: 0.1834 Accuracy: 73.00%\n",
            "Loss: 0.2104 Accuracy: 73.01%\n",
            "Loss: 0.1225 Accuracy: 73.06%\n",
            "Loss: 0.2238 Accuracy: 73.06%\n",
            "Loss: 0.1429 Accuracy: 73.08%\n",
            "Loss: 0.1843 Accuracy: 73.10%\n",
            "Loss: 0.1926 Accuracy: 73.09%\n",
            "Loss: 0.2340 Accuracy: 73.11%\n",
            "Loss: 0.2438 Accuracy: 73.09%\n",
            "Loss: 0.1176 Accuracy: 73.11%\n",
            "Loss: 0.2495 Accuracy: 73.07%\n",
            "Loss: 0.1343 Accuracy: 73.09%\n",
            "Loss: 0.2627 Accuracy: 73.07%\n",
            "Loss: 0.1273 Accuracy: 73.15%\n",
            "Loss: 0.1693 Accuracy: 73.19%\n",
            "Loss: 0.1958 Accuracy: 73.13%\n",
            "Loss: 0.2175 Accuracy: 73.10%\n",
            "Loss: 0.2047 Accuracy: 73.08%\n",
            "Loss: 0.1822 Accuracy: 73.10%\n",
            "Loss: 0.2540 Accuracy: 73.11%\n",
            "Loss: 0.1092 Accuracy: 73.16%\n",
            "Loss: 0.2978 Accuracy: 73.09%\n",
            "Loss: 0.1651 Accuracy: 73.08%\n",
            "Loss: 0.1715 Accuracy: 73.07%\n",
            "Loss: 0.1354 Accuracy: 73.09%\n",
            "Loss: 0.1154 Accuracy: 73.14%\n",
            "Loss: 0.3817 Accuracy: 73.09%\n",
            "Loss: 0.1899 Accuracy: 73.11%\n",
            "Loss: 0.1869 Accuracy: 73.08%\n",
            "Loss: 0.1132 Accuracy: 73.10%\n",
            "Loss: 0.2453 Accuracy: 73.06%\n",
            "Loss: 0.1707 Accuracy: 73.06%\n",
            "Loss: 0.2524 Accuracy: 73.04%\n",
            "Loss: 0.1099 Accuracy: 73.10%\n",
            "Loss: 0.1705 Accuracy: 73.11%\n",
            "Loss: 0.2301 Accuracy: 73.07%\n",
            "Loss: 0.2612 Accuracy: 73.07%\n",
            "Loss: 0.1477 Accuracy: 73.10%\n",
            "Loss: 0.2639 Accuracy: 73.01%\n",
            "Loss: 0.2358 Accuracy: 73.03%\n",
            "Loss: 0.1718 Accuracy: 73.03%\n",
            "Loss: 0.2816 Accuracy: 73.01%\n",
            "Loss: 0.2148 Accuracy: 73.05%\n",
            "Loss: 0.1851 Accuracy: 73.02%\n",
            "Loss: 0.2145 Accuracy: 73.01%\n",
            "Loss: 0.1657 Accuracy: 73.01%\n",
            "Loss: 0.2297 Accuracy: 73.02%\n",
            "Loss: 0.1512 Accuracy: 73.04%\n",
            "Loss: 0.1884 Accuracy: 73.08%\n",
            "Loss: 0.2449 Accuracy: 73.07%\n",
            "Loss: 0.1838 Accuracy: 73.12%\n",
            "Loss: 0.2891 Accuracy: 73.09%\n",
            "Final Test Accuracy: 73.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Cifar-10 이미지를 아래의 이미지처럼 4개씩 묶고 분류할 수 있는 모델을 구현하세요."
      ],
      "metadata": {
        "id": "X9vfA1geXZTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU69GmIkEs8W",
        "outputId": "374f4f49-8195-44ed-8510-2174bea18e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 62, 62]             896\n",
            "         MaxPool2d-2           [-1, 32, 31, 31]               0\n",
            "            Conv2d-3           [-1, 64, 29, 29]          18,496\n",
            "         MaxPool2d-4           [-1, 64, 14, 14]               0\n",
            "            Conv2d-5          [-1, 128, 12, 12]          73,856\n",
            "         MaxPool2d-6            [-1, 128, 6, 6]               0\n",
            "            Conv2d-7            [-1, 256, 4, 4]         295,168\n",
            "         MaxPool2d-8            [-1, 256, 2, 2]               0\n",
            "            Linear-9                  [-1, 512]         524,800\n",
            "          Dropout-10                  [-1, 512]               0\n",
            "           Linear-11                  [-1, 128]          65,664\n",
            "           Linear-12                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 980,170\n",
            "Trainable params: 980,170\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.05\n",
            "Forward/backward pass size (MB): 1.90\n",
            "Params size (MB): 3.74\n",
            "Estimated Total Size (MB): 5.69\n",
            "----------------------------------------------------------------\n",
            "Epoch [1/50], Loss: 0.6439, Accuracy: 65.35%\n",
            "Epoch [2/50], Loss: 0.6218, Accuracy: 66.02%\n",
            "Epoch [3/50], Loss: 0.6330, Accuracy: 67.01%\n",
            "Epoch [4/50], Loss: 0.5902, Accuracy: 68.16%\n",
            "Epoch [5/50], Loss: 0.5773, Accuracy: 69.54%\n",
            "Epoch [6/50], Loss: 0.5481, Accuracy: 71.48%\n",
            "Epoch [7/50], Loss: 0.4865, Accuracy: 73.72%\n",
            "Epoch [8/50], Loss: 0.4658, Accuracy: 75.72%\n",
            "Epoch [9/50], Loss: 0.4707, Accuracy: 77.71%\n",
            "Epoch [10/50], Loss: 0.4764, Accuracy: 79.49%\n",
            "Epoch [11/50], Loss: 0.4394, Accuracy: 80.67%\n",
            "Epoch [12/50], Loss: 0.4102, Accuracy: 82.02%\n",
            "Epoch [13/50], Loss: 0.3104, Accuracy: 83.06%\n",
            "Epoch [14/50], Loss: 0.3422, Accuracy: 84.25%\n",
            "Epoch [15/50], Loss: 0.3083, Accuracy: 85.09%\n",
            "Epoch [16/50], Loss: 0.2909, Accuracy: 86.08%\n",
            "Epoch [17/50], Loss: 0.2852, Accuracy: 87.01%\n",
            "Epoch [18/50], Loss: 0.2331, Accuracy: 87.65%\n",
            "Epoch [19/50], Loss: 0.2351, Accuracy: 88.46%\n",
            "Epoch [20/50], Loss: 0.2972, Accuracy: 89.25%\n",
            "Epoch [21/50], Loss: 0.2453, Accuracy: 89.89%\n",
            "Epoch [22/50], Loss: 0.2677, Accuracy: 90.64%\n",
            "Epoch [23/50], Loss: 0.2154, Accuracy: 91.15%\n",
            "Epoch [24/50], Loss: 0.2437, Accuracy: 91.68%\n",
            "Epoch [25/50], Loss: 0.1669, Accuracy: 92.38%\n",
            "Epoch [26/50], Loss: 0.1562, Accuracy: 92.95%\n",
            "Epoch [27/50], Loss: 0.1219, Accuracy: 93.41%\n",
            "Epoch [28/50], Loss: 0.2199, Accuracy: 94.09%\n",
            "Epoch [29/50], Loss: 0.1676, Accuracy: 94.59%\n",
            "Epoch [30/50], Loss: 0.1604, Accuracy: 94.78%\n",
            "Epoch [31/50], Loss: 0.1449, Accuracy: 95.34%\n",
            "Epoch [32/50], Loss: 0.1097, Accuracy: 95.68%\n",
            "Epoch [33/50], Loss: 0.1342, Accuracy: 95.80%\n",
            "Epoch [34/50], Loss: 0.1196, Accuracy: 96.15%\n",
            "Epoch [35/50], Loss: 0.1237, Accuracy: 96.20%\n",
            "Epoch [36/50], Loss: 0.0974, Accuracy: 96.42%\n",
            "Epoch [37/50], Loss: 0.1026, Accuracy: 96.64%\n",
            "Epoch [38/50], Loss: 0.0681, Accuracy: 96.77%\n",
            "Epoch [39/50], Loss: 0.0354, Accuracy: 97.03%\n",
            "Epoch [40/50], Loss: 0.0431, Accuracy: 97.18%\n",
            "Epoch [41/50], Loss: 0.1055, Accuracy: 97.21%\n",
            "Epoch [42/50], Loss: 0.0337, Accuracy: 97.28%\n",
            "Epoch [43/50], Loss: 0.0847, Accuracy: 97.45%\n",
            "Epoch [44/50], Loss: 0.0713, Accuracy: 97.44%\n",
            "Epoch [45/50], Loss: 0.0674, Accuracy: 97.54%\n",
            "Epoch [46/50], Loss: 0.0965, Accuracy: 97.65%\n",
            "Epoch [47/50], Loss: 0.0739, Accuracy: 97.61%\n",
            "Epoch [48/50], Loss: 0.0595, Accuracy: 97.84%\n",
            "Epoch [49/50], Loss: 0.0442, Accuracy: 97.86%\n",
            "Epoch [50/50], Loss: 0.0288, Accuracy: 97.94%\n",
            "훈련 완료\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# 데이터 전처리 및 데이터 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# CIFAR-10 데이터셋 로드\n",
        "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, original_dataset):\n",
        "        self.original_dataset = original_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.original_dataset) // 4  # 4개씩 묶어서 사용\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1, label1 = self.original_dataset[idx * 4]\n",
        "        img2, label2 = self.original_dataset[idx * 4 + 1]\n",
        "        img3, label3 = self.original_dataset[idx * 4 + 2]\n",
        "        img4, label4 = self.original_dataset[idx * 4 + 3]\n",
        "\n",
        "        concatenated_img1 = torch.cat([img1, img2], dim=2)\n",
        "        concatenated_img2 = torch.cat([img3, img4], dim=2)\n",
        "        concatenated_img = torch.cat([concatenated_img1, concatenated_img2], dim=1)\n",
        "        concatenated_lbl = torch.tensor([label1, label2, label3, label4])\n",
        "\n",
        "        return concatenated_img, concatenated_lbl\n",
        "\n",
        "# 이어붙인 데이터셋으로 DataLoader 생성\n",
        "batch_size = 64\n",
        "\n",
        "concatenated_train_data = CIFAR10Dataset(cifar_train)\n",
        "train_loader = DataLoader(concatenated_train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "''' Image 확인용\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "'''\n",
        "\n",
        "#신경망 정의\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3)\n",
        "        self.drop1 = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(256 * 2 * 2, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 모델, 손실 함수, 최적화 함수 정의\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "summary(model, input_size=(3, 64, 64)) # Summary 출력\n",
        "\n",
        "# 훈련\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        new_labels = [[0.0 for tt in range(10)] for t in range(len(labels))] # 1 0 0 1 0 0 1 1 0 0 형태로 만들어주기 위한 배열\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            new_labels[t][labels[t][j]] = 1.0\n",
        "        labels = torch.tensor(new_labels)\n",
        "        labels = labels.to(device)\n",
        "        loss = criterion(outputs, labels) #loss 계산(BCEWithLogitsLoss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred_soft = torch.sigmoid(outputs)\n",
        "        pred_binary = pred_soft.gt(0.5).type(outputs.type())\n",
        "        total += (labels == labels).sum().item()\n",
        "        correct += (pred_binary == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {correct / total * 100:.2f}%') # Loss, Accuracy 출력\n",
        "\n",
        "print('훈련 완료')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCVv_12alq78",
        "outputId": "8ed577df-e9bf-4f1a-a362-b82101d6fe29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0254 Accuracy: 98.22%\n",
            "Loss: 0.0254 Accuracy2: 99.06%\n",
            "Loss: 0.0213 Accuracy: 98.68%\n",
            "Loss: 0.0213 Accuracy2: 99.38%\n",
            "Loss: 0.0151 Accuracy: 98.81%\n",
            "Loss: 0.0151 Accuracy2: 99.48%\n",
            "Loss: 0.0195 Accuracy: 98.54%\n",
            "Loss: 0.0195 Accuracy2: 99.34%\n",
            "Loss: 0.0125 Accuracy: 98.57%\n",
            "Loss: 0.0125 Accuracy2: 99.38%\n",
            "Loss: 0.0303 Accuracy: 98.73%\n",
            "Loss: 0.0303 Accuracy2: 99.40%\n",
            "Loss: 0.0140 Accuracy: 98.78%\n",
            "Loss: 0.0140 Accuracy2: 99.40%\n",
            "Loss: 0.0210 Accuracy: 98.82%\n",
            "Loss: 0.0210 Accuracy2: 99.41%\n",
            "Loss: 0.0089 Accuracy: 98.90%\n",
            "Loss: 0.0089 Accuracy2: 99.44%\n",
            "Loss: 0.0147 Accuracy: 98.96%\n",
            "Loss: 0.0147 Accuracy2: 99.44%\n",
            "Loss: 0.0311 Accuracy: 98.93%\n",
            "Loss: 0.0311 Accuracy2: 99.39%\n",
            "Loss: 0.0219 Accuracy: 98.88%\n",
            "Loss: 0.0219 Accuracy2: 99.38%\n",
            "Loss: 0.0316 Accuracy: 98.79%\n",
            "Loss: 0.0316 Accuracy2: 99.34%\n",
            "Loss: 0.0153 Accuracy: 98.81%\n",
            "Loss: 0.0153 Accuracy2: 99.35%\n",
            "Loss: 0.0243 Accuracy: 98.80%\n",
            "Loss: 0.0243 Accuracy2: 99.35%\n",
            "Loss: 0.0220 Accuracy: 98.84%\n",
            "Loss: 0.0220 Accuracy2: 99.35%\n",
            "Loss: 0.0216 Accuracy: 98.86%\n",
            "Loss: 0.0216 Accuracy2: 99.36%\n",
            "Loss: 0.0110 Accuracy: 98.92%\n",
            "Loss: 0.0110 Accuracy2: 99.38%\n",
            "Loss: 0.0209 Accuracy: 98.95%\n",
            "Loss: 0.0209 Accuracy2: 99.38%\n",
            "Loss: 0.0141 Accuracy: 98.98%\n",
            "Loss: 0.0141 Accuracy2: 99.41%\n",
            "Loss: 0.0210 Accuracy: 99.01%\n",
            "Loss: 0.0210 Accuracy2: 99.41%\n",
            "Loss: 0.0199 Accuracy: 98.97%\n",
            "Loss: 0.0199 Accuracy2: 99.41%\n",
            "Loss: 0.0220 Accuracy: 98.99%\n",
            "Loss: 0.0220 Accuracy2: 99.40%\n",
            "Loss: 0.0181 Accuracy: 99.02%\n",
            "Loss: 0.0181 Accuracy2: 99.41%\n",
            "Loss: 0.0184 Accuracy: 99.04%\n",
            "Loss: 0.0184 Accuracy2: 99.41%\n",
            "Loss: 0.0144 Accuracy: 99.04%\n",
            "Loss: 0.0144 Accuracy2: 99.42%\n",
            "Loss: 0.0269 Accuracy: 99.01%\n",
            "Loss: 0.0269 Accuracy2: 99.42%\n",
            "Loss: 0.0295 Accuracy: 99.01%\n",
            "Loss: 0.0295 Accuracy2: 99.41%\n",
            "Loss: 0.0133 Accuracy: 99.02%\n",
            "Loss: 0.0133 Accuracy2: 99.42%\n",
            "Loss: 0.0378 Accuracy: 99.02%\n",
            "Loss: 0.0378 Accuracy2: 99.41%\n",
            "Loss: 0.0135 Accuracy: 99.05%\n",
            "Loss: 0.0135 Accuracy2: 99.42%\n",
            "Loss: 0.0122 Accuracy: 99.07%\n",
            "Loss: 0.0122 Accuracy2: 99.42%\n",
            "Loss: 0.0154 Accuracy: 99.05%\n",
            "Loss: 0.0154 Accuracy2: 99.42%\n",
            "Loss: 0.0272 Accuracy: 99.07%\n",
            "Loss: 0.0272 Accuracy2: 99.42%\n",
            "Loss: 0.0221 Accuracy: 99.06%\n",
            "Loss: 0.0221 Accuracy2: 99.40%\n",
            "Loss: 0.0151 Accuracy: 99.07%\n",
            "Loss: 0.0151 Accuracy2: 99.41%\n",
            "Loss: 0.0303 Accuracy: 99.03%\n",
            "Loss: 0.0303 Accuracy2: 99.40%\n",
            "Loss: 0.0196 Accuracy: 99.02%\n",
            "Loss: 0.0196 Accuracy2: 99.40%\n",
            "Loss: 0.0203 Accuracy: 99.04%\n",
            "Loss: 0.0203 Accuracy2: 99.40%\n",
            "Loss: 0.0266 Accuracy: 99.04%\n",
            "Loss: 0.0266 Accuracy2: 99.39%\n",
            "Loss: 0.0254 Accuracy: 99.03%\n",
            "Loss: 0.0254 Accuracy2: 99.38%\n",
            "Loss: 0.0202 Accuracy: 99.04%\n",
            "Loss: 0.0202 Accuracy2: 99.38%\n",
            "Loss: 0.0371 Accuracy: 98.99%\n",
            "Loss: 0.0371 Accuracy2: 99.36%\n",
            "Loss: 0.0243 Accuracy: 99.01%\n",
            "Loss: 0.0243 Accuracy2: 99.36%\n",
            "Loss: 0.0114 Accuracy: 99.03%\n",
            "Loss: 0.0114 Accuracy2: 99.38%\n",
            "Loss: 0.0115 Accuracy: 99.04%\n",
            "Loss: 0.0115 Accuracy2: 99.39%\n",
            "Loss: 0.0091 Accuracy: 99.06%\n",
            "Loss: 0.0091 Accuracy2: 99.40%\n",
            "Loss: 0.0410 Accuracy: 99.03%\n",
            "Loss: 0.0410 Accuracy2: 99.38%\n",
            "Loss: 0.0130 Accuracy: 99.04%\n",
            "Loss: 0.0130 Accuracy2: 99.38%\n",
            "Loss: 0.0132 Accuracy: 99.05%\n",
            "Loss: 0.0132 Accuracy2: 99.39%\n",
            "Loss: 0.0218 Accuracy: 99.06%\n",
            "Loss: 0.0218 Accuracy2: 99.39%\n",
            "Loss: 0.0194 Accuracy: 99.05%\n",
            "Loss: 0.0194 Accuracy2: 99.39%\n",
            "Loss: 0.0174 Accuracy: 99.05%\n",
            "Loss: 0.0174 Accuracy2: 99.39%\n",
            "Loss: 0.0191 Accuracy: 99.05%\n",
            "Loss: 0.0191 Accuracy2: 99.40%\n",
            "Loss: 0.0282 Accuracy: 99.05%\n",
            "Loss: 0.0282 Accuracy2: 99.39%\n",
            "Loss: 0.0154 Accuracy: 99.05%\n",
            "Loss: 0.0154 Accuracy2: 99.40%\n",
            "Loss: 0.0230 Accuracy: 99.06%\n",
            "Loss: 0.0230 Accuracy2: 99.40%\n",
            "Loss: 0.0180 Accuracy: 99.08%\n",
            "Loss: 0.0180 Accuracy2: 99.40%\n",
            "Loss: 0.0209 Accuracy: 99.07%\n",
            "Loss: 0.0209 Accuracy2: 99.39%\n",
            "Loss: 0.0143 Accuracy: 99.07%\n",
            "Loss: 0.0143 Accuracy2: 99.40%\n",
            "Loss: 0.0168 Accuracy: 99.06%\n",
            "Loss: 0.0168 Accuracy2: 99.40%\n",
            "Loss: 0.0145 Accuracy: 99.08%\n",
            "Loss: 0.0145 Accuracy2: 99.40%\n",
            "Loss: 0.0219 Accuracy: 99.08%\n",
            "Loss: 0.0219 Accuracy2: 99.40%\n",
            "Loss: 0.0145 Accuracy: 99.09%\n",
            "Loss: 0.0145 Accuracy2: 99.41%\n",
            "Loss: 0.0323 Accuracy: 99.09%\n",
            "Loss: 0.0323 Accuracy2: 99.40%\n",
            "Loss: 0.0154 Accuracy: 99.10%\n",
            "Loss: 0.0154 Accuracy2: 99.40%\n",
            "Loss: 0.0174 Accuracy: 99.10%\n",
            "Loss: 0.0174 Accuracy2: 99.40%\n",
            "Loss: 0.0105 Accuracy: 99.11%\n",
            "Loss: 0.0105 Accuracy2: 99.41%\n",
            "Loss: 0.0214 Accuracy: 99.09%\n",
            "Loss: 0.0214 Accuracy2: 99.40%\n",
            "Loss: 0.0155 Accuracy: 99.09%\n",
            "Loss: 0.0155 Accuracy2: 99.40%\n",
            "Loss: 0.0192 Accuracy: 99.09%\n",
            "Loss: 0.0192 Accuracy2: 99.40%\n",
            "Loss: 0.0185 Accuracy: 99.09%\n",
            "Loss: 0.0185 Accuracy2: 99.40%\n",
            "Loss: 0.0108 Accuracy: 99.10%\n",
            "Loss: 0.0108 Accuracy2: 99.41%\n",
            "Loss: 0.0206 Accuracy: 99.10%\n",
            "Loss: 0.0206 Accuracy2: 99.41%\n",
            "Loss: 0.0175 Accuracy: 99.10%\n",
            "Loss: 0.0175 Accuracy2: 99.41%\n",
            "Loss: 0.0195 Accuracy: 99.10%\n",
            "Loss: 0.0195 Accuracy2: 99.41%\n",
            "Loss: 0.0204 Accuracy: 99.10%\n",
            "Loss: 0.0204 Accuracy2: 99.41%\n",
            "Loss: 0.0195 Accuracy: 99.10%\n",
            "Loss: 0.0195 Accuracy2: 99.41%\n",
            "Loss: 0.0224 Accuracy: 99.10%\n",
            "Loss: 0.0224 Accuracy2: 99.41%\n",
            "Loss: 0.0226 Accuracy: 99.10%\n",
            "Loss: 0.0226 Accuracy2: 99.40%\n",
            "Loss: 0.0106 Accuracy: 99.11%\n",
            "Loss: 0.0106 Accuracy2: 99.41%\n",
            "Loss: 0.0188 Accuracy: 99.10%\n",
            "Loss: 0.0188 Accuracy2: 99.41%\n",
            "Loss: 0.0129 Accuracy: 99.11%\n",
            "Loss: 0.0129 Accuracy2: 99.41%\n",
            "Loss: 0.0126 Accuracy: 99.12%\n",
            "Loss: 0.0126 Accuracy2: 99.42%\n",
            "Loss: 0.0244 Accuracy: 99.11%\n",
            "Loss: 0.0244 Accuracy2: 99.41%\n",
            "Loss: 0.0272 Accuracy: 99.10%\n",
            "Loss: 0.0272 Accuracy2: 99.41%\n",
            "Loss: 0.0213 Accuracy: 99.11%\n",
            "Loss: 0.0213 Accuracy2: 99.41%\n",
            "Loss: 0.0272 Accuracy: 99.11%\n",
            "Loss: 0.0272 Accuracy2: 99.41%\n",
            "Loss: 0.0152 Accuracy: 99.11%\n",
            "Loss: 0.0152 Accuracy2: 99.41%\n",
            "Loss: 0.0161 Accuracy: 99.11%\n",
            "Loss: 0.0161 Accuracy2: 99.41%\n",
            "Loss: 0.0237 Accuracy: 99.11%\n",
            "Loss: 0.0237 Accuracy2: 99.41%\n",
            "Loss: 0.0202 Accuracy: 99.11%\n",
            "Loss: 0.0202 Accuracy2: 99.40%\n",
            "Loss: 0.0169 Accuracy: 99.11%\n",
            "Loss: 0.0169 Accuracy2: 99.41%\n",
            "Loss: 0.0176 Accuracy: 99.10%\n",
            "Loss: 0.0176 Accuracy2: 99.40%\n",
            "Loss: 0.0269 Accuracy: 99.09%\n",
            "Loss: 0.0269 Accuracy2: 99.40%\n",
            "Loss: 0.0193 Accuracy: 99.08%\n",
            "Loss: 0.0193 Accuracy2: 99.40%\n",
            "Loss: 0.0115 Accuracy: 99.09%\n",
            "Loss: 0.0115 Accuracy2: 99.40%\n",
            "Loss: 0.0198 Accuracy: 99.09%\n",
            "Loss: 0.0198 Accuracy2: 99.41%\n",
            "Loss: 0.0178 Accuracy: 99.09%\n",
            "Loss: 0.0178 Accuracy2: 99.40%\n",
            "Loss: 0.0115 Accuracy: 99.08%\n",
            "Loss: 0.0115 Accuracy2: 99.41%\n",
            "Loss: 0.0258 Accuracy: 99.08%\n",
            "Loss: 0.0258 Accuracy2: 99.40%\n",
            "Loss: 0.0162 Accuracy: 99.08%\n",
            "Loss: 0.0162 Accuracy2: 99.40%\n",
            "Loss: 0.0134 Accuracy: 99.09%\n",
            "Loss: 0.0134 Accuracy2: 99.41%\n",
            "Loss: 0.0175 Accuracy: 99.08%\n",
            "Loss: 0.0175 Accuracy2: 99.41%\n",
            "Loss: 0.0204 Accuracy: 99.09%\n",
            "Loss: 0.0204 Accuracy2: 99.40%\n",
            "Loss: 0.0253 Accuracy: 99.08%\n",
            "Loss: 0.0253 Accuracy2: 99.40%\n",
            "Loss: 0.0248 Accuracy: 99.08%\n",
            "Loss: 0.0248 Accuracy2: 99.40%\n",
            "Loss: 0.0216 Accuracy: 99.08%\n",
            "Loss: 0.0216 Accuracy2: 99.40%\n",
            "Loss: 0.0187 Accuracy: 99.08%\n",
            "Loss: 0.0187 Accuracy2: 99.40%\n",
            "Loss: 0.0128 Accuracy: 99.08%\n",
            "Loss: 0.0128 Accuracy2: 99.40%\n",
            "Loss: 0.0243 Accuracy: 99.08%\n",
            "Loss: 0.0243 Accuracy2: 99.40%\n",
            "Loss: 0.0147 Accuracy: 99.08%\n",
            "Loss: 0.0147 Accuracy2: 99.40%\n",
            "Loss: 0.0190 Accuracy: 99.08%\n",
            "Loss: 0.0190 Accuracy2: 99.40%\n",
            "Loss: 0.0093 Accuracy: 99.08%\n",
            "Loss: 0.0093 Accuracy2: 99.40%\n",
            "Loss: 0.0149 Accuracy: 99.08%\n",
            "Loss: 0.0149 Accuracy2: 99.40%\n",
            "Loss: 0.0307 Accuracy: 99.08%\n",
            "Loss: 0.0307 Accuracy2: 99.40%\n",
            "Loss: 0.0247 Accuracy: 99.07%\n",
            "Loss: 0.0247 Accuracy2: 99.39%\n",
            "Loss: 0.0185 Accuracy: 99.07%\n",
            "Loss: 0.0185 Accuracy2: 99.39%\n",
            "Loss: 0.0152 Accuracy: 99.07%\n",
            "Loss: 0.0152 Accuracy2: 99.39%\n",
            "Loss: 0.0221 Accuracy: 99.07%\n",
            "Loss: 0.0221 Accuracy2: 99.39%\n",
            "Loss: 0.0130 Accuracy: 99.07%\n",
            "Loss: 0.0130 Accuracy2: 99.39%\n",
            "Loss: 0.0215 Accuracy: 99.07%\n",
            "Loss: 0.0215 Accuracy2: 99.39%\n",
            "Loss: 0.0113 Accuracy: 99.08%\n",
            "Loss: 0.0113 Accuracy2: 99.40%\n",
            "Loss: 0.0131 Accuracy: 99.08%\n",
            "Loss: 0.0131 Accuracy2: 99.40%\n",
            "Loss: 0.0125 Accuracy: 99.09%\n",
            "Loss: 0.0125 Accuracy2: 99.40%\n",
            "Loss: 0.0090 Accuracy: 99.09%\n",
            "Loss: 0.0090 Accuracy2: 99.41%\n",
            "Loss: 0.0245 Accuracy: 99.09%\n",
            "Loss: 0.0245 Accuracy2: 99.40%\n",
            "Loss: 0.0180 Accuracy: 99.10%\n",
            "Loss: 0.0180 Accuracy2: 99.41%\n",
            "Loss: 0.0232 Accuracy: 99.09%\n",
            "Loss: 0.0232 Accuracy2: 99.40%\n",
            "Loss: 0.0267 Accuracy: 99.08%\n",
            "Loss: 0.0267 Accuracy2: 99.40%\n",
            "Loss: 0.0249 Accuracy: 99.09%\n",
            "Loss: 0.0249 Accuracy2: 99.40%\n",
            "Loss: 0.0102 Accuracy: 99.09%\n",
            "Loss: 0.0102 Accuracy2: 99.40%\n",
            "Loss: 0.0237 Accuracy: 99.08%\n",
            "Loss: 0.0237 Accuracy2: 99.40%\n",
            "Loss: 0.0246 Accuracy: 99.07%\n",
            "Loss: 0.0246 Accuracy2: 99.40%\n",
            "Loss: 0.0198 Accuracy: 99.07%\n",
            "Loss: 0.0198 Accuracy2: 99.39%\n",
            "Loss: 0.0260 Accuracy: 99.06%\n",
            "Loss: 0.0260 Accuracy2: 99.39%\n",
            "Loss: 0.0172 Accuracy: 99.06%\n",
            "Loss: 0.0172 Accuracy2: 99.39%\n",
            "Loss: 0.0126 Accuracy: 99.07%\n",
            "Loss: 0.0126 Accuracy2: 99.40%\n",
            "Loss: 0.0287 Accuracy: 99.06%\n",
            "Loss: 0.0287 Accuracy2: 99.39%\n",
            "Loss: 0.0181 Accuracy: 99.06%\n",
            "Loss: 0.0181 Accuracy2: 99.39%\n",
            "Loss: 0.0283 Accuracy: 99.06%\n",
            "Loss: 0.0283 Accuracy2: 99.39%\n",
            "Loss: 0.0102 Accuracy: 99.07%\n",
            "Loss: 0.0102 Accuracy2: 99.40%\n",
            "Loss: 0.0102 Accuracy: 99.07%\n",
            "Loss: 0.0102 Accuracy2: 99.40%\n",
            "Loss: 0.0169 Accuracy: 99.07%\n",
            "Loss: 0.0169 Accuracy2: 99.40%\n",
            "Loss: 0.0191 Accuracy: 99.07%\n",
            "Loss: 0.0191 Accuracy2: 99.40%\n",
            "Loss: 0.0190 Accuracy: 99.07%\n",
            "Loss: 0.0190 Accuracy2: 99.40%\n",
            "Loss: 0.0224 Accuracy: 99.06%\n",
            "Loss: 0.0224 Accuracy2: 99.40%\n",
            "Loss: 0.0264 Accuracy: 99.05%\n",
            "Loss: 0.0264 Accuracy2: 99.39%\n",
            "Loss: 0.0226 Accuracy: 99.05%\n",
            "Loss: 0.0226 Accuracy2: 99.39%\n",
            "Loss: 0.0222 Accuracy: 99.05%\n",
            "Loss: 0.0222 Accuracy2: 99.39%\n",
            "Loss: 0.0105 Accuracy: 99.05%\n",
            "Loss: 0.0105 Accuracy2: 99.40%\n",
            "Loss: 0.0199 Accuracy: 99.05%\n",
            "Loss: 0.0199 Accuracy2: 99.40%\n",
            "Loss: 0.0153 Accuracy: 99.05%\n",
            "Loss: 0.0153 Accuracy2: 99.40%\n",
            "Loss: 0.0269 Accuracy: 99.05%\n",
            "Loss: 0.0269 Accuracy2: 99.39%\n",
            "Loss: 0.0198 Accuracy: 99.05%\n",
            "Loss: 0.0198 Accuracy2: 99.40%\n",
            "Loss: 0.0169 Accuracy: 99.05%\n",
            "Loss: 0.0169 Accuracy2: 99.40%\n",
            "Loss: 0.0253 Accuracy: 99.05%\n",
            "Loss: 0.0253 Accuracy2: 99.39%\n",
            "Loss: 0.0118 Accuracy: 99.06%\n",
            "Loss: 0.0118 Accuracy2: 99.40%\n",
            "Loss: 0.0206 Accuracy: 99.06%\n",
            "Loss: 0.0206 Accuracy2: 99.40%\n",
            "Loss: 0.0136 Accuracy: 99.06%\n",
            "Loss: 0.0136 Accuracy2: 99.40%\n",
            "Loss: 0.0133 Accuracy: 99.07%\n",
            "Loss: 0.0133 Accuracy2: 99.40%\n",
            "Loss: 0.0295 Accuracy: 99.06%\n",
            "Loss: 0.0295 Accuracy2: 99.40%\n",
            "Loss: 0.0254 Accuracy: 99.05%\n",
            "Loss: 0.0254 Accuracy2: 99.39%\n",
            "Loss: 0.0336 Accuracy: 99.05%\n",
            "Loss: 0.0336 Accuracy2: 99.39%\n",
            "Loss: 0.0214 Accuracy: 99.05%\n",
            "Loss: 0.0214 Accuracy2: 99.39%\n",
            "Loss: 0.0293 Accuracy: 99.05%\n",
            "Loss: 0.0293 Accuracy2: 99.39%\n",
            "Loss: 0.0109 Accuracy: 99.05%\n",
            "Loss: 0.0109 Accuracy2: 99.39%\n",
            "Loss: 0.0140 Accuracy: 99.06%\n",
            "Loss: 0.0140 Accuracy2: 99.39%\n",
            "Loss: 0.0359 Accuracy: 99.05%\n",
            "Loss: 0.0359 Accuracy2: 99.39%\n",
            "Loss: 0.0223 Accuracy: 99.05%\n",
            "Loss: 0.0223 Accuracy2: 99.39%\n",
            "Loss: 0.0144 Accuracy: 99.05%\n",
            "Loss: 0.0144 Accuracy2: 99.39%\n",
            "Loss: 0.0297 Accuracy: 99.05%\n",
            "Loss: 0.0297 Accuracy2: 99.39%\n",
            "Loss: 0.0117 Accuracy: 99.05%\n",
            "Loss: 0.0117 Accuracy2: 99.39%\n",
            "Loss: 0.0223 Accuracy: 99.05%\n",
            "Loss: 0.0223 Accuracy2: 99.39%\n",
            "Loss: 0.0239 Accuracy: 99.05%\n",
            "Loss: 0.0239 Accuracy2: 99.39%\n",
            "Loss: 0.0230 Accuracy: 99.05%\n",
            "Loss: 0.0230 Accuracy2: 99.39%\n",
            "Loss: 0.0130 Accuracy: 99.05%\n",
            "Loss: 0.0130 Accuracy2: 99.39%\n",
            "Loss: 0.0205 Accuracy: 99.05%\n",
            "Loss: 0.0205 Accuracy2: 99.39%\n",
            "Loss: 0.0193 Accuracy: 99.06%\n",
            "Loss: 0.0193 Accuracy2: 99.39%\n",
            "Loss: 0.0148 Accuracy: 99.06%\n",
            "Loss: 0.0148 Accuracy2: 99.39%\n",
            "Loss: 0.0353 Accuracy: 99.06%\n",
            "Loss: 0.0353 Accuracy2: 99.39%\n",
            "Loss: 0.0281 Accuracy: 99.05%\n",
            "Loss: 0.0281 Accuracy2: 99.38%\n",
            "Loss: 0.0191 Accuracy: 99.05%\n",
            "Loss: 0.0191 Accuracy2: 99.38%\n",
            "Loss: 0.0172 Accuracy: 99.06%\n",
            "Loss: 0.0172 Accuracy2: 99.39%\n",
            "Loss: 0.0115 Accuracy: 99.06%\n",
            "Loss: 0.0115 Accuracy2: 99.39%\n",
            "Loss: 0.0206 Accuracy: 99.06%\n",
            "Loss: 0.0206 Accuracy2: 99.39%\n",
            "Loss: 0.0215 Accuracy: 99.06%\n",
            "Loss: 0.0215 Accuracy2: 99.39%\n",
            "Loss: 0.0161 Accuracy: 99.06%\n",
            "Loss: 0.0161 Accuracy2: 99.39%\n",
            "Loss: 0.0167 Accuracy: 99.06%\n",
            "Loss: 0.0167 Accuracy2: 99.39%\n",
            "Loss: 0.0269 Accuracy: 99.06%\n",
            "Loss: 0.0269 Accuracy2: 99.39%\n",
            "Loss: 0.0234 Accuracy: 99.06%\n",
            "Loss: 0.0234 Accuracy2: 99.38%\n",
            "Loss: 0.0167 Accuracy: 99.06%\n",
            "Loss: 0.0167 Accuracy2: 99.38%\n",
            "Loss: 0.0219 Accuracy: 99.05%\n",
            "Loss: 0.0219 Accuracy2: 99.38%\n",
            "Loss: 0.0149 Accuracy: 99.05%\n",
            "Loss: 0.0149 Accuracy2: 99.38%\n",
            "Loss: 0.0259 Accuracy: 99.05%\n",
            "Loss: 0.0259 Accuracy2: 99.38%\n",
            "Loss: 0.0107 Accuracy: 99.05%\n",
            "Loss: 0.0107 Accuracy2: 99.38%\n",
            "Final Test Accuracy: 99.05%\n",
            "Final Test Accuracy2: 99.38%\n"
          ]
        }
      ],
      "source": [
        "# 테스트\n",
        "model.eval().to(device)\n",
        "correct = 0\n",
        "total = 0\n",
        "correct2 = 0\n",
        "total2 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        pred_soft = torch.sigmoid(outputs)\n",
        "        pred_binary = pred_soft.gt(0.5).type(outputs.type())\n",
        "        new_labels = [[0.0 for tt in range(10)] for t in range(len(labels))]\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            new_labels[t][labels[t][j]] = 1.0\n",
        "        labels = torch.tensor(new_labels)\n",
        "        labels = labels.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total2 += (labels == labels).sum().item()\n",
        "        correct2 += (pred_binary == labels).sum().item()\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            if (labels[t][j] == 1.0):\n",
        "              total += 1\n",
        "              if (pred_binary[t][j] == 1.0):\n",
        "                correct += 1\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy: {(correct / total * 100):.2f}%') # Label이 1일 때, 추정값도 1인 개수 비율\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy2: {(correct2 / total2 * 100):.2f}%') # 추정값 중 얼마나 맞았는지에 대한 비율 (Label과 추정값이 얼마나 같은지에 대한 비율)\n",
        "\n",
        "accuracy = correct / total\n",
        "accuracy2 = correct2 / total2\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%') # Label이 1일 때, 추정값도 1인 개수 비율\n",
        "print(f'Final Test Accuracy2: {accuracy2 * 100:.2f}%') # 추정값 중 얼마나 맞았는지에 대한 비율 (Label과 추정값이 얼마나 같은지에 대한 비율)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSaunygyFSjq",
        "outputId": "967f1a56-0e17-44e3-ebc0-e88ef9df9a9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Loss: 1.2582 Accuracy: 61.19%\n",
            "Loss: 1.2582 Accuracy2: 75.78%\n",
            "Loss: 1.5690 Accuracy: 62.12%\n",
            "Loss: 1.5690 Accuracy2: 75.55%\n",
            "Loss: 1.2715 Accuracy: 63.00%\n",
            "Loss: 1.2715 Accuracy2: 76.04%\n",
            "Loss: 1.3139 Accuracy: 62.83%\n",
            "Loss: 1.3139 Accuracy2: 75.59%\n",
            "Loss: 1.4461 Accuracy: 62.13%\n",
            "Loss: 1.4461 Accuracy2: 75.31%\n",
            "Loss: 1.3845 Accuracy: 62.17%\n",
            "Loss: 1.3845 Accuracy2: 75.34%\n",
            "Loss: 1.2780 Accuracy: 62.37%\n",
            "Loss: 1.2780 Accuracy2: 75.38%\n",
            "Loss: 1.3836 Accuracy: 62.07%\n",
            "Loss: 1.3836 Accuracy2: 75.12%\n",
            "Loss: 1.2114 Accuracy: 62.53%\n",
            "Loss: 1.2114 Accuracy2: 75.47%\n",
            "Loss: 1.1772 Accuracy: 62.77%\n",
            "Loss: 1.1772 Accuracy2: 75.47%\n",
            "Loss: 1.3592 Accuracy: 63.12%\n",
            "Loss: 1.3592 Accuracy2: 75.60%\n",
            "Loss: 1.3120 Accuracy: 63.08%\n",
            "Loss: 1.3120 Accuracy2: 75.60%\n",
            "Loss: 1.4866 Accuracy: 62.95%\n",
            "Loss: 1.4866 Accuracy2: 75.46%\n",
            "Loss: 1.1726 Accuracy: 63.17%\n",
            "Loss: 1.1726 Accuracy2: 75.71%\n",
            "Loss: 1.4586 Accuracy: 63.26%\n",
            "Loss: 1.4586 Accuracy2: 75.81%\n",
            "Loss: 1.5372 Accuracy: 63.00%\n",
            "Loss: 1.5372 Accuracy2: 75.67%\n",
            "Loss: 1.4806 Accuracy: 62.80%\n",
            "Loss: 1.4806 Accuracy2: 75.62%\n",
            "Loss: 1.4666 Accuracy: 62.61%\n",
            "Loss: 1.4666 Accuracy2: 75.52%\n",
            "Loss: 1.2411 Accuracy: 62.69%\n",
            "Loss: 1.2411 Accuracy2: 75.48%\n",
            "Loss: 1.2672 Accuracy: 62.63%\n",
            "Loss: 1.2672 Accuracy2: 75.47%\n",
            "Loss: 1.5556 Accuracy: 62.48%\n",
            "Loss: 1.5556 Accuracy2: 75.34%\n",
            "Loss: 1.4139 Accuracy: 62.45%\n",
            "Loss: 1.4139 Accuracy2: 75.33%\n",
            "Loss: 1.1027 Accuracy: 62.65%\n",
            "Loss: 1.1027 Accuracy2: 75.46%\n",
            "Loss: 1.4373 Accuracy: 62.53%\n",
            "Loss: 1.4373 Accuracy2: 75.35%\n",
            "Loss: 1.2951 Accuracy: 62.49%\n",
            "Loss: 1.2951 Accuracy2: 75.33%\n",
            "Loss: 1.4167 Accuracy: 62.32%\n",
            "Loss: 1.4167 Accuracy2: 75.29%\n",
            "Loss: 1.2587 Accuracy: 62.30%\n",
            "Loss: 1.2587 Accuracy2: 75.29%\n",
            "Loss: 1.4886 Accuracy: 62.08%\n",
            "Loss: 1.4886 Accuracy2: 75.22%\n",
            "Loss: 1.2397 Accuracy: 62.17%\n",
            "Loss: 1.2397 Accuracy2: 75.30%\n",
            "Loss: 1.0358 Accuracy: 62.29%\n",
            "Loss: 1.0358 Accuracy2: 75.38%\n",
            "Loss: 1.4450 Accuracy: 62.29%\n",
            "Loss: 1.4450 Accuracy2: 75.31%\n",
            "Loss: 1.2774 Accuracy: 62.37%\n",
            "Loss: 1.2774 Accuracy2: 75.32%\n",
            "Loss: 1.4503 Accuracy: 62.30%\n",
            "Loss: 1.4503 Accuracy2: 75.28%\n",
            "Loss: 1.5739 Accuracy: 62.20%\n",
            "Loss: 1.5739 Accuracy2: 75.20%\n",
            "Loss: 1.3297 Accuracy: 62.24%\n",
            "Loss: 1.3297 Accuracy2: 75.19%\n",
            "Loss: 1.1791 Accuracy: 62.33%\n",
            "Loss: 1.1791 Accuracy2: 75.28%\n",
            "Loss: 1.2225 Accuracy: 62.29%\n",
            "Loss: 1.2225 Accuracy2: 75.28%\n",
            "Loss: 1.3390 Accuracy: 62.28%\n",
            "Loss: 1.3390 Accuracy2: 75.27%\n",
            "Loss: 1.3578 Accuracy: 62.20%\n",
            "Loss: 1.3578 Accuracy2: 75.22%\n",
            "Loss: 1.1060 Accuracy: 62.21%\n",
            "Loss: 1.1060 Accuracy2: 75.23%\n",
            "Final Test Accuracy: 62.21%\n",
            "Final Test Accuracy2: 75.23%\n"
          ]
        }
      ],
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "concatenated_test_data = CIFAR10Dataset(testset)\n",
        "# 이어붙인 데이터셋으로 DataLoader 생성\n",
        "batch_size = 64\n",
        "test_loader = DataLoader(concatenated_test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "correct2 = 0\n",
        "total2 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        pred_soft = torch.sigmoid(outputs)\n",
        "        pred_binary = pred_soft.gt(0.5).type(outputs.type())\n",
        "        new_labels = [[0.0 for tt in range(10)] for t in range(len(labels))]\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            new_labels[t][labels[t][j]] = 1.0\n",
        "        labels = torch.tensor(new_labels)\n",
        "        labels = labels.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total2 += (labels == labels).sum().item()\n",
        "        correct2 += (pred_binary == labels).sum().item()\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            if (labels[t][j] == 1.0):\n",
        "              total += 1\n",
        "              if (pred_binary[t][j] == 1.0):\n",
        "                correct += 1\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy: {(correct / total * 100):.2f}%') # Label이 1일 때, 추정값도 1인 개수 비율\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy2: {(correct2 / total2 * 100):.2f}%') # 추정값 중 얼마나 맞았는지에 대한 비율 (Label과 추정값이 얼마나 같은지에 대한 비율)\n",
        "\n",
        "accuracy = correct / total\n",
        "accuracy2 = correct2 / total2\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%') # Label이 1일 때, 추정값도 1인 개수 비율\n",
        "print(f'Final Test Accuracy2: {accuracy2 * 100:.2f}%') # 추정값 중 얼마나 맞았는지에 대한 비율 (Label과 추정값이 얼마나 같은지에 대한 비율)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2-2. MNIST 데이터를 4장씩 아래의 형태로 묶어서 분류할 수 있는 모델을 만들고 학습 및 테스트를 할 수 있는 코드를 제출하세요."
      ],
      "metadata": {
        "id": "RKnt_-q4rMkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchsummary import summary\n",
        "import torch.nn.functional as F\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485],\n",
        "                                 std=[0.229])\n",
        "\n",
        "# 데이터 전처리 및 데이터 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# CIFAR-10 데이터셋 로드\n",
        "cifar_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, original_dataset):\n",
        "        self.original_dataset = original_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.original_dataset) // 4  # 4개씩 묶어서 사용\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img1, label1 = self.original_dataset[idx * 4]\n",
        "        img2, label2 = self.original_dataset[idx * 4 + 1]\n",
        "        img3, label3 = self.original_dataset[idx * 4 + 2]\n",
        "        img4, label4 = self.original_dataset[idx * 4 + 3]\n",
        "\n",
        "        concatenated_img1 = torch.cat([img1, img2], dim=2)\n",
        "        concatenated_img2 = torch.cat([img3, img4], dim=2)\n",
        "        concatenated_img = torch.cat([concatenated_img1, concatenated_img2], dim=1)\n",
        "        concatenated_lbl = torch.tensor([label1, label2, label3, label4])\n",
        "\n",
        "        return concatenated_img, concatenated_lbl\n",
        "\n",
        "concatenated_train_data = CIFAR10Dataset(cifar_train)\n",
        "# 이어붙인 데이터셋으로 DataLoader 생성\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(concatenated_train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3)\n",
        "        self.drop1 = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 모델, 손실 함수, 최적화 함수 정의\n",
        "model = SimpleCNN().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "summary(model, input_size=(1, 56, 56))\n",
        "\n",
        "# 훈련\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        new_labels = [[0.0 for tt in range(10)] for t in range(len(labels))] # 1 0 0 1 0 0 1 1 0 0 형태로 만들어주기 위한 배열\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            new_labels[t][labels[t][j]] = 1.0\n",
        "        labels = torch.tensor(new_labels)\n",
        "        labels = labels.to(device)\n",
        "        loss = criterion(outputs, labels) #loss 계산(BCEWithLogitsLoss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pred_soft = torch.sigmoid(outputs)\n",
        "        pred_binary = pred_soft.gt(0.5).type(outputs.type())\n",
        "        total += (labels == labels).sum().item()\n",
        "        correct += (pred_binary == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Accuracy: {correct / total * 100:.2f}%') # Loss, Accuracy 출력\n",
        "\n",
        "print('훈련 완료')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISHiOTa4rMl1",
        "outputId": "766dd57d-5d87-41c0-c469-93e59a202551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 360596643.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 42444181.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 141948880.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 19680298.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 54, 54]             320\n",
            "         MaxPool2d-2           [-1, 32, 27, 27]               0\n",
            "            Conv2d-3           [-1, 64, 25, 25]          18,496\n",
            "         MaxPool2d-4           [-1, 64, 12, 12]               0\n",
            "            Conv2d-5          [-1, 128, 10, 10]          73,856\n",
            "         MaxPool2d-6            [-1, 128, 5, 5]               0\n",
            "            Conv2d-7            [-1, 256, 3, 3]         295,168\n",
            "         MaxPool2d-8            [-1, 256, 1, 1]               0\n",
            "            Linear-9                  [-1, 128]          32,896\n",
            "          Dropout-10                  [-1, 128]               0\n",
            "           Linear-11                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 422,026\n",
            "Trainable params: 422,026\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.41\n",
            "Params size (MB): 1.61\n",
            "Estimated Total Size (MB): 3.03\n",
            "----------------------------------------------------------------\n",
            "Epoch [1/50], Loss: 0.5215, Accuracy: 68.62%\n",
            "Epoch [2/50], Loss: 0.3314, Accuracy: 75.59%\n",
            "Epoch [3/50], Loss: 0.1669, Accuracy: 80.08%\n",
            "Epoch [4/50], Loss: 0.1892, Accuracy: 83.15%\n",
            "Epoch [5/50], Loss: 0.1826, Accuracy: 85.40%\n",
            "Epoch [6/50], Loss: 0.1542, Accuracy: 87.07%\n",
            "Epoch [7/50], Loss: 0.1024, Accuracy: 88.39%\n",
            "Epoch [8/50], Loss: 0.0970, Accuracy: 89.43%\n",
            "Epoch [9/50], Loss: 0.1409, Accuracy: 90.30%\n",
            "Epoch [10/50], Loss: 0.0434, Accuracy: 91.03%\n",
            "Epoch [11/50], Loss: 0.0615, Accuracy: 91.65%\n",
            "Epoch [12/50], Loss: 0.0509, Accuracy: 92.19%\n",
            "Epoch [13/50], Loss: 0.0962, Accuracy: 92.66%\n",
            "Epoch [14/50], Loss: 0.0599, Accuracy: 93.08%\n",
            "Epoch [15/50], Loss: 0.0683, Accuracy: 93.45%\n",
            "Epoch [16/50], Loss: 0.0191, Accuracy: 93.78%\n",
            "Epoch [17/50], Loss: 0.0371, Accuracy: 94.08%\n",
            "Epoch [18/50], Loss: 0.0215, Accuracy: 94.35%\n",
            "Epoch [19/50], Loss: 0.0357, Accuracy: 94.60%\n",
            "Epoch [20/50], Loss: 0.0835, Accuracy: 94.82%\n",
            "Epoch [21/50], Loss: 0.0203, Accuracy: 95.02%\n",
            "Epoch [22/50], Loss: 0.0153, Accuracy: 95.21%\n",
            "Epoch [23/50], Loss: 0.0219, Accuracy: 95.38%\n",
            "Epoch [24/50], Loss: 0.0182, Accuracy: 95.54%\n",
            "Epoch [25/50], Loss: 0.0239, Accuracy: 95.69%\n",
            "Epoch [26/50], Loss: 0.0084, Accuracy: 95.83%\n",
            "Epoch [27/50], Loss: 0.0087, Accuracy: 95.96%\n",
            "Epoch [28/50], Loss: 0.0175, Accuracy: 96.08%\n",
            "Epoch [29/50], Loss: 0.0132, Accuracy: 96.19%\n",
            "Epoch [30/50], Loss: 0.0277, Accuracy: 96.30%\n",
            "Epoch [31/50], Loss: 0.0356, Accuracy: 96.40%\n",
            "Epoch [32/50], Loss: 0.0126, Accuracy: 96.49%\n",
            "Epoch [33/50], Loss: 0.0274, Accuracy: 96.58%\n",
            "Epoch [34/50], Loss: 0.0175, Accuracy: 96.66%\n",
            "Epoch [35/50], Loss: 0.0332, Accuracy: 96.74%\n",
            "Epoch [36/50], Loss: 0.0138, Accuracy: 96.82%\n",
            "Epoch [37/50], Loss: 0.0116, Accuracy: 96.89%\n",
            "Epoch [38/50], Loss: 0.0023, Accuracy: 96.96%\n",
            "Epoch [39/50], Loss: 0.0190, Accuracy: 97.02%\n",
            "Epoch [40/50], Loss: 0.0245, Accuracy: 97.09%\n",
            "Epoch [41/50], Loss: 0.0259, Accuracy: 97.15%\n",
            "Epoch [42/50], Loss: 0.0055, Accuracy: 97.20%\n",
            "Epoch [43/50], Loss: 0.0022, Accuracy: 97.26%\n",
            "Epoch [44/50], Loss: 0.0074, Accuracy: 97.31%\n",
            "Epoch [45/50], Loss: 0.0356, Accuracy: 97.35%\n",
            "Epoch [46/50], Loss: 0.0273, Accuracy: 97.40%\n",
            "Epoch [47/50], Loss: 0.0208, Accuracy: 97.45%\n",
            "Epoch [48/50], Loss: 0.0035, Accuracy: 97.49%\n",
            "Epoch [49/50], Loss: 0.0241, Accuracy: 97.53%\n",
            "Epoch [50/50], Loss: 0.0141, Accuracy: 97.57%\n",
            "훈련 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트\n",
        "model.eval().to(device)\n",
        "correct = 0\n",
        "total = 0\n",
        "correct2 = 0\n",
        "total2 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        pred_soft = torch.sigmoid(outputs)\n",
        "        pred_binary = pred_soft.gt(0.5).type(outputs.type())\n",
        "        new_labels = [[0.0 for tt in range(10)] for t in range(len(labels))]\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            new_labels[t][labels[t][j]] = 1.0\n",
        "        labels = torch.tensor(new_labels)\n",
        "        labels = labels.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total2 += (labels == labels).sum().item()\n",
        "        correct2 += (pred_binary == labels).sum().item()\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            if (labels[t][j] == 1.0):\n",
        "              total += 1\n",
        "              if (pred_binary[t][j] == 1.0):\n",
        "                correct += 1\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy: {(correct / total * 100):.2f}%') # Label이 1일 때, 추정값도 1인 개수 비율\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy2: {(correct2 / total2 * 100):.2f}%') # 추정값 중 얼마나 맞았는지에 대한 비율 (Label과 추정값이 얼마나 같은지에 대한 비율)\n",
        "\n",
        "accuracy = correct / total\n",
        "accuracy2 = correct2 / total2\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%') # Label이 1일 때, 추정값도 1인 개수 비율\n",
        "print(f'Final Test Accuracy2: {accuracy2 * 100:.2f}%') # 추정값 중 얼마나 맞았는지에 대한 비율 (Label과 추정값이 얼마나 같은지에 대한 비율)"
      ],
      "metadata": {
        "id": "QYYBmDLyrRtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77deb91-2994-4460-b2d6-e5faef32b21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0286 Accuracy: 99.09%\n",
            "Loss: 0.0286 Accuracy2: 99.69%\n",
            "Loss: 0.0241 Accuracy: 98.86%\n",
            "Loss: 0.0241 Accuracy2: 99.53%\n",
            "Loss: 0.0127 Accuracy: 99.10%\n",
            "Loss: 0.0127 Accuracy2: 99.58%\n",
            "Loss: 0.0029 Accuracy: 99.32%\n",
            "Loss: 0.0029 Accuracy2: 99.65%\n",
            "Loss: 0.0027 Accuracy: 99.36%\n",
            "Loss: 0.0027 Accuracy2: 99.69%\n",
            "Loss: 0.0061 Accuracy: 99.39%\n",
            "Loss: 0.0061 Accuracy2: 99.69%\n",
            "Loss: 0.0001 Accuracy: 99.48%\n",
            "Loss: 0.0001 Accuracy2: 99.73%\n",
            "Loss: 0.0078 Accuracy: 99.55%\n",
            "Loss: 0.0078 Accuracy2: 99.75%\n",
            "Loss: 0.0086 Accuracy: 99.54%\n",
            "Loss: 0.0086 Accuracy2: 99.76%\n",
            "Loss: 0.0026 Accuracy: 99.59%\n",
            "Loss: 0.0026 Accuracy2: 99.77%\n",
            "Loss: 0.0017 Accuracy: 99.59%\n",
            "Loss: 0.0017 Accuracy2: 99.77%\n",
            "Loss: 0.0003 Accuracy: 99.62%\n",
            "Loss: 0.0003 Accuracy2: 99.79%\n",
            "Loss: 0.0005 Accuracy: 99.65%\n",
            "Loss: 0.0005 Accuracy2: 99.81%\n",
            "Loss: 0.0080 Accuracy: 99.64%\n",
            "Loss: 0.0080 Accuracy2: 99.80%\n",
            "Loss: 0.0011 Accuracy: 99.67%\n",
            "Loss: 0.0011 Accuracy2: 99.81%\n",
            "Loss: 0.0024 Accuracy: 99.69%\n",
            "Loss: 0.0024 Accuracy2: 99.81%\n",
            "Loss: 0.0013 Accuracy: 99.71%\n",
            "Loss: 0.0013 Accuracy2: 99.83%\n",
            "Loss: 0.0013 Accuracy: 99.72%\n",
            "Loss: 0.0013 Accuracy2: 99.84%\n",
            "Loss: 0.0012 Accuracy: 99.74%\n",
            "Loss: 0.0012 Accuracy2: 99.84%\n",
            "Loss: 0.0025 Accuracy: 99.75%\n",
            "Loss: 0.0025 Accuracy2: 99.84%\n",
            "Loss: 0.0167 Accuracy: 99.74%\n",
            "Loss: 0.0167 Accuracy2: 99.84%\n",
            "Loss: 0.0003 Accuracy: 99.75%\n",
            "Loss: 0.0003 Accuracy2: 99.85%\n",
            "Loss: 0.0009 Accuracy: 99.76%\n",
            "Loss: 0.0009 Accuracy2: 99.86%\n",
            "Loss: 0.0009 Accuracy: 99.77%\n",
            "Loss: 0.0009 Accuracy2: 99.86%\n",
            "Loss: 0.0029 Accuracy: 99.76%\n",
            "Loss: 0.0029 Accuracy2: 99.86%\n",
            "Loss: 0.0013 Accuracy: 99.77%\n",
            "Loss: 0.0013 Accuracy2: 99.87%\n",
            "Loss: 0.0002 Accuracy: 99.78%\n",
            "Loss: 0.0002 Accuracy2: 99.87%\n",
            "Loss: 0.0071 Accuracy: 99.76%\n",
            "Loss: 0.0071 Accuracy2: 99.87%\n",
            "Loss: 0.0011 Accuracy: 99.77%\n",
            "Loss: 0.0011 Accuracy2: 99.87%\n",
            "Loss: 0.0053 Accuracy: 99.74%\n",
            "Loss: 0.0053 Accuracy2: 99.86%\n",
            "Loss: 0.0029 Accuracy: 99.74%\n",
            "Loss: 0.0029 Accuracy2: 99.86%\n",
            "Loss: 0.0010 Accuracy: 99.75%\n",
            "Loss: 0.0010 Accuracy2: 99.87%\n",
            "Loss: 0.0068 Accuracy: 99.75%\n",
            "Loss: 0.0068 Accuracy2: 99.87%\n",
            "Loss: 0.0006 Accuracy: 99.76%\n",
            "Loss: 0.0006 Accuracy2: 99.87%\n",
            "Loss: 0.0017 Accuracy: 99.77%\n",
            "Loss: 0.0017 Accuracy2: 99.88%\n",
            "Loss: 0.0082 Accuracy: 99.76%\n",
            "Loss: 0.0082 Accuracy2: 99.87%\n",
            "Loss: 0.0004 Accuracy: 99.77%\n",
            "Loss: 0.0004 Accuracy2: 99.88%\n",
            "Loss: 0.0005 Accuracy: 99.77%\n",
            "Loss: 0.0005 Accuracy2: 99.88%\n",
            "Loss: 0.0005 Accuracy: 99.78%\n",
            "Loss: 0.0005 Accuracy2: 99.88%\n",
            "Loss: 0.0010 Accuracy: 99.78%\n",
            "Loss: 0.0010 Accuracy2: 99.89%\n",
            "Loss: 0.0001 Accuracy: 99.79%\n",
            "Loss: 0.0001 Accuracy2: 99.89%\n",
            "Loss: 0.0031 Accuracy: 99.78%\n",
            "Loss: 0.0031 Accuracy2: 99.89%\n",
            "Loss: 0.0002 Accuracy: 99.79%\n",
            "Loss: 0.0002 Accuracy2: 99.89%\n",
            "Loss: 0.0001 Accuracy: 99.79%\n",
            "Loss: 0.0001 Accuracy2: 99.89%\n",
            "Loss: 0.0006 Accuracy: 99.80%\n",
            "Loss: 0.0006 Accuracy2: 99.90%\n",
            "Loss: 0.0005 Accuracy: 99.80%\n",
            "Loss: 0.0005 Accuracy2: 99.90%\n",
            "Loss: 0.0001 Accuracy: 99.81%\n",
            "Loss: 0.0001 Accuracy2: 99.90%\n",
            "Loss: 0.0023 Accuracy: 99.81%\n",
            "Loss: 0.0023 Accuracy2: 99.90%\n",
            "Loss: 0.0001 Accuracy: 99.82%\n",
            "Loss: 0.0001 Accuracy2: 99.90%\n",
            "Loss: 0.0008 Accuracy: 99.82%\n",
            "Loss: 0.0008 Accuracy2: 99.90%\n",
            "Loss: 0.0017 Accuracy: 99.81%\n",
            "Loss: 0.0017 Accuracy2: 99.90%\n",
            "Loss: 0.0021 Accuracy: 99.82%\n",
            "Loss: 0.0021 Accuracy2: 99.90%\n",
            "Loss: 0.0009 Accuracy: 99.82%\n",
            "Loss: 0.0009 Accuracy2: 99.91%\n",
            "Loss: 0.0124 Accuracy: 99.82%\n",
            "Loss: 0.0124 Accuracy2: 99.90%\n",
            "Loss: 0.0004 Accuracy: 99.82%\n",
            "Loss: 0.0004 Accuracy2: 99.91%\n",
            "Loss: 0.0055 Accuracy: 99.82%\n",
            "Loss: 0.0055 Accuracy2: 99.91%\n",
            "Loss: 0.0003 Accuracy: 99.83%\n",
            "Loss: 0.0003 Accuracy2: 99.91%\n",
            "Loss: 0.0025 Accuracy: 99.82%\n",
            "Loss: 0.0025 Accuracy2: 99.91%\n",
            "Loss: 0.0062 Accuracy: 99.82%\n",
            "Loss: 0.0062 Accuracy2: 99.90%\n",
            "Loss: 0.0016 Accuracy: 99.82%\n",
            "Loss: 0.0016 Accuracy2: 99.91%\n",
            "Loss: 0.0024 Accuracy: 99.82%\n",
            "Loss: 0.0024 Accuracy2: 99.91%\n",
            "Loss: 0.0082 Accuracy: 99.82%\n",
            "Loss: 0.0082 Accuracy2: 99.90%\n",
            "Loss: 0.0020 Accuracy: 99.81%\n",
            "Loss: 0.0020 Accuracy2: 99.90%\n",
            "Loss: 0.0002 Accuracy: 99.82%\n",
            "Loss: 0.0002 Accuracy2: 99.90%\n",
            "Loss: 0.0031 Accuracy: 99.81%\n",
            "Loss: 0.0031 Accuracy2: 99.90%\n",
            "Loss: 0.0003 Accuracy: 99.82%\n",
            "Loss: 0.0003 Accuracy2: 99.91%\n",
            "Loss: 0.0031 Accuracy: 99.81%\n",
            "Loss: 0.0031 Accuracy2: 99.90%\n",
            "Loss: 0.0003 Accuracy: 99.81%\n",
            "Loss: 0.0003 Accuracy2: 99.91%\n",
            "Loss: 0.0081 Accuracy: 99.81%\n",
            "Loss: 0.0081 Accuracy2: 99.90%\n",
            "Loss: 0.0004 Accuracy: 99.81%\n",
            "Loss: 0.0004 Accuracy2: 99.91%\n",
            "Loss: 0.0011 Accuracy: 99.82%\n",
            "Loss: 0.0011 Accuracy2: 99.91%\n",
            "Loss: 0.0011 Accuracy: 99.82%\n",
            "Loss: 0.0011 Accuracy2: 99.91%\n",
            "Loss: 0.0012 Accuracy: 99.82%\n",
            "Loss: 0.0012 Accuracy2: 99.91%\n",
            "Loss: 0.0033 Accuracy: 99.82%\n",
            "Loss: 0.0033 Accuracy2: 99.91%\n",
            "Loss: 0.0003 Accuracy: 99.82%\n",
            "Loss: 0.0003 Accuracy2: 99.91%\n",
            "Loss: 0.0004 Accuracy: 99.82%\n",
            "Loss: 0.0004 Accuracy2: 99.91%\n",
            "Loss: 0.0092 Accuracy: 99.81%\n",
            "Loss: 0.0092 Accuracy2: 99.91%\n",
            "Loss: 0.0129 Accuracy: 99.81%\n",
            "Loss: 0.0129 Accuracy2: 99.90%\n",
            "Loss: 0.0127 Accuracy: 99.81%\n",
            "Loss: 0.0127 Accuracy2: 99.90%\n",
            "Loss: 0.0005 Accuracy: 99.81%\n",
            "Loss: 0.0005 Accuracy2: 99.90%\n",
            "Loss: 0.0012 Accuracy: 99.81%\n",
            "Loss: 0.0012 Accuracy2: 99.90%\n",
            "Loss: 0.0010 Accuracy: 99.81%\n",
            "Loss: 0.0010 Accuracy2: 99.90%\n",
            "Loss: 0.0051 Accuracy: 99.81%\n",
            "Loss: 0.0051 Accuracy2: 99.90%\n",
            "Loss: 0.0114 Accuracy: 99.81%\n",
            "Loss: 0.0114 Accuracy2: 99.90%\n",
            "Loss: 0.0100 Accuracy: 99.79%\n",
            "Loss: 0.0100 Accuracy2: 99.89%\n",
            "Loss: 0.0041 Accuracy: 99.79%\n",
            "Loss: 0.0041 Accuracy2: 99.89%\n",
            "Loss: 0.0017 Accuracy: 99.79%\n",
            "Loss: 0.0017 Accuracy2: 99.89%\n",
            "Loss: 0.0015 Accuracy: 99.79%\n",
            "Loss: 0.0015 Accuracy2: 99.89%\n",
            "Loss: 0.0031 Accuracy: 99.79%\n",
            "Loss: 0.0031 Accuracy2: 99.89%\n",
            "Loss: 0.0002 Accuracy: 99.79%\n",
            "Loss: 0.0002 Accuracy2: 99.89%\n",
            "Loss: 0.0046 Accuracy: 99.79%\n",
            "Loss: 0.0046 Accuracy2: 99.89%\n",
            "Loss: 0.0093 Accuracy: 99.79%\n",
            "Loss: 0.0093 Accuracy2: 99.89%\n",
            "Loss: 0.0007 Accuracy: 99.79%\n",
            "Loss: 0.0007 Accuracy2: 99.89%\n",
            "Loss: 0.0042 Accuracy: 99.79%\n",
            "Loss: 0.0042 Accuracy2: 99.89%\n",
            "Loss: 0.0010 Accuracy: 99.79%\n",
            "Loss: 0.0010 Accuracy2: 99.89%\n",
            "Loss: 0.0013 Accuracy: 99.79%\n",
            "Loss: 0.0013 Accuracy2: 99.89%\n",
            "Loss: 0.0003 Accuracy: 99.80%\n",
            "Loss: 0.0003 Accuracy2: 99.89%\n",
            "Loss: 0.0024 Accuracy: 99.79%\n",
            "Loss: 0.0024 Accuracy2: 99.89%\n",
            "Loss: 0.0016 Accuracy: 99.79%\n",
            "Loss: 0.0016 Accuracy2: 99.89%\n",
            "Loss: 0.0017 Accuracy: 99.79%\n",
            "Loss: 0.0017 Accuracy2: 99.89%\n",
            "Loss: 0.0003 Accuracy: 99.79%\n",
            "Loss: 0.0003 Accuracy2: 99.89%\n",
            "Loss: 0.0002 Accuracy: 99.80%\n",
            "Loss: 0.0002 Accuracy2: 99.90%\n",
            "Loss: 0.0055 Accuracy: 99.79%\n",
            "Loss: 0.0055 Accuracy2: 99.90%\n",
            "Loss: 0.0013 Accuracy: 99.80%\n",
            "Loss: 0.0013 Accuracy2: 99.90%\n",
            "Loss: 0.0043 Accuracy: 99.79%\n",
            "Loss: 0.0043 Accuracy2: 99.89%\n",
            "Loss: 0.0006 Accuracy: 99.80%\n",
            "Loss: 0.0006 Accuracy2: 99.90%\n",
            "Loss: 0.0095 Accuracy: 99.79%\n",
            "Loss: 0.0095 Accuracy2: 99.89%\n",
            "Loss: 0.0001 Accuracy: 99.80%\n",
            "Loss: 0.0001 Accuracy2: 99.90%\n",
            "Loss: 0.0132 Accuracy: 99.79%\n",
            "Loss: 0.0132 Accuracy2: 99.90%\n",
            "Loss: 0.0007 Accuracy: 99.80%\n",
            "Loss: 0.0007 Accuracy2: 99.90%\n",
            "Loss: 0.0038 Accuracy: 99.80%\n",
            "Loss: 0.0038 Accuracy2: 99.90%\n",
            "Loss: 0.0051 Accuracy: 99.79%\n",
            "Loss: 0.0051 Accuracy2: 99.90%\n",
            "Loss: 0.0005 Accuracy: 99.80%\n",
            "Loss: 0.0005 Accuracy2: 99.90%\n",
            "Loss: 0.0086 Accuracy: 99.79%\n",
            "Loss: 0.0086 Accuracy2: 99.90%\n",
            "Loss: 0.0003 Accuracy: 99.80%\n",
            "Loss: 0.0003 Accuracy2: 99.90%\n",
            "Loss: 0.0003 Accuracy: 99.80%\n",
            "Loss: 0.0003 Accuracy2: 99.90%\n",
            "Loss: 0.0013 Accuracy: 99.80%\n",
            "Loss: 0.0013 Accuracy2: 99.90%\n",
            "Loss: 0.0008 Accuracy: 99.80%\n",
            "Loss: 0.0008 Accuracy2: 99.90%\n",
            "Loss: 0.0007 Accuracy: 99.80%\n",
            "Loss: 0.0007 Accuracy2: 99.90%\n",
            "Loss: 0.0010 Accuracy: 99.80%\n",
            "Loss: 0.0010 Accuracy2: 99.90%\n",
            "Loss: 0.0083 Accuracy: 99.80%\n",
            "Loss: 0.0083 Accuracy2: 99.90%\n",
            "Loss: 0.0055 Accuracy: 99.80%\n",
            "Loss: 0.0055 Accuracy2: 99.90%\n",
            "Loss: 0.0047 Accuracy: 99.80%\n",
            "Loss: 0.0047 Accuracy2: 99.90%\n",
            "Loss: 0.0032 Accuracy: 99.80%\n",
            "Loss: 0.0032 Accuracy2: 99.90%\n",
            "Loss: 0.0004 Accuracy: 99.80%\n",
            "Loss: 0.0004 Accuracy2: 99.90%\n",
            "Loss: 0.0003 Accuracy: 99.80%\n",
            "Loss: 0.0003 Accuracy2: 99.90%\n",
            "Loss: 0.0025 Accuracy: 99.80%\n",
            "Loss: 0.0025 Accuracy2: 99.90%\n",
            "Loss: 0.0001 Accuracy: 99.80%\n",
            "Loss: 0.0001 Accuracy2: 99.90%\n",
            "Loss: 0.0027 Accuracy: 99.80%\n",
            "Loss: 0.0027 Accuracy2: 99.90%\n",
            "Loss: 0.0004 Accuracy: 99.80%\n",
            "Loss: 0.0004 Accuracy2: 99.90%\n",
            "Loss: 0.0006 Accuracy: 99.80%\n",
            "Loss: 0.0006 Accuracy2: 99.90%\n",
            "Loss: 0.0083 Accuracy: 99.80%\n",
            "Loss: 0.0083 Accuracy2: 99.90%\n",
            "Loss: 0.0011 Accuracy: 99.80%\n",
            "Loss: 0.0011 Accuracy2: 99.90%\n",
            "Loss: 0.0056 Accuracy: 99.80%\n",
            "Loss: 0.0056 Accuracy2: 99.90%\n",
            "Loss: 0.0002 Accuracy: 99.80%\n",
            "Loss: 0.0002 Accuracy2: 99.90%\n",
            "Loss: 0.0054 Accuracy: 99.80%\n",
            "Loss: 0.0054 Accuracy2: 99.90%\n",
            "Loss: 0.0004 Accuracy: 99.80%\n",
            "Loss: 0.0004 Accuracy2: 99.90%\n",
            "Loss: 0.0012 Accuracy: 99.80%\n",
            "Loss: 0.0012 Accuracy2: 99.90%\n",
            "Loss: 0.0011 Accuracy: 99.81%\n",
            "Loss: 0.0011 Accuracy2: 99.90%\n",
            "Loss: 0.0041 Accuracy: 99.80%\n",
            "Loss: 0.0041 Accuracy2: 99.90%\n",
            "Loss: 0.0008 Accuracy: 99.81%\n",
            "Loss: 0.0008 Accuracy2: 99.90%\n",
            "Loss: 0.0117 Accuracy: 99.80%\n",
            "Loss: 0.0117 Accuracy2: 99.90%\n",
            "Loss: 0.0042 Accuracy: 99.80%\n",
            "Loss: 0.0042 Accuracy2: 99.90%\n",
            "Loss: 0.0002 Accuracy: 99.80%\n",
            "Loss: 0.0002 Accuracy2: 99.90%\n",
            "Loss: 0.0006 Accuracy: 99.80%\n",
            "Loss: 0.0006 Accuracy2: 99.91%\n",
            "Loss: 0.0002 Accuracy: 99.81%\n",
            "Loss: 0.0002 Accuracy2: 99.91%\n",
            "Loss: 0.0004 Accuracy: 99.81%\n",
            "Loss: 0.0004 Accuracy2: 99.91%\n",
            "Loss: 0.0003 Accuracy: 99.81%\n",
            "Loss: 0.0003 Accuracy2: 99.91%\n",
            "Loss: 0.0013 Accuracy: 99.81%\n",
            "Loss: 0.0013 Accuracy2: 99.91%\n",
            "Loss: 0.0032 Accuracy: 99.80%\n",
            "Loss: 0.0032 Accuracy2: 99.91%\n",
            "Loss: 0.0024 Accuracy: 99.80%\n",
            "Loss: 0.0024 Accuracy2: 99.91%\n",
            "Loss: 0.0602 Accuracy: 99.80%\n",
            "Loss: 0.0602 Accuracy2: 99.90%\n",
            "Loss: 0.0032 Accuracy: 99.80%\n",
            "Loss: 0.0032 Accuracy2: 99.90%\n",
            "Loss: 0.0050 Accuracy: 99.80%\n",
            "Loss: 0.0050 Accuracy2: 99.90%\n",
            "Loss: 0.0004 Accuracy: 99.80%\n",
            "Loss: 0.0004 Accuracy2: 99.90%\n",
            "Loss: 0.0021 Accuracy: 99.80%\n",
            "Loss: 0.0021 Accuracy2: 99.90%\n",
            "Loss: 0.0058 Accuracy: 99.80%\n",
            "Loss: 0.0058 Accuracy2: 99.90%\n",
            "Loss: 0.0020 Accuracy: 99.80%\n",
            "Loss: 0.0020 Accuracy2: 99.90%\n",
            "Loss: 0.0062 Accuracy: 99.80%\n",
            "Loss: 0.0062 Accuracy2: 99.90%\n",
            "Loss: 0.0046 Accuracy: 99.80%\n",
            "Loss: 0.0046 Accuracy2: 99.90%\n",
            "Loss: 0.0018 Accuracy: 99.80%\n",
            "Loss: 0.0018 Accuracy2: 99.90%\n",
            "Loss: 0.0047 Accuracy: 99.80%\n",
            "Loss: 0.0047 Accuracy2: 99.90%\n",
            "Loss: 0.0007 Accuracy: 99.80%\n",
            "Loss: 0.0007 Accuracy2: 99.90%\n",
            "Loss: 0.0034 Accuracy: 99.80%\n",
            "Loss: 0.0034 Accuracy2: 99.90%\n",
            "Loss: 0.0167 Accuracy: 99.80%\n",
            "Loss: 0.0167 Accuracy2: 99.90%\n",
            "Loss: 0.0008 Accuracy: 99.80%\n",
            "Loss: 0.0008 Accuracy2: 99.90%\n",
            "Loss: 0.0008 Accuracy: 99.80%\n",
            "Loss: 0.0008 Accuracy2: 99.90%\n",
            "Loss: 0.0025 Accuracy: 99.80%\n",
            "Loss: 0.0025 Accuracy2: 99.90%\n",
            "Loss: 0.0032 Accuracy: 99.79%\n",
            "Loss: 0.0032 Accuracy2: 99.90%\n",
            "Loss: 0.0015 Accuracy: 99.80%\n",
            "Loss: 0.0015 Accuracy2: 99.90%\n",
            "Loss: 0.0006 Accuracy: 99.80%\n",
            "Loss: 0.0006 Accuracy2: 99.90%\n",
            "Loss: 0.0090 Accuracy: 99.80%\n",
            "Loss: 0.0090 Accuracy2: 99.90%\n",
            "Loss: 0.0007 Accuracy: 99.80%\n",
            "Loss: 0.0007 Accuracy2: 99.90%\n",
            "Loss: 0.0035 Accuracy: 99.80%\n",
            "Loss: 0.0035 Accuracy2: 99.90%\n",
            "Loss: 0.0009 Accuracy: 99.80%\n",
            "Loss: 0.0009 Accuracy2: 99.90%\n",
            "Loss: 0.0127 Accuracy: 99.80%\n",
            "Loss: 0.0127 Accuracy2: 99.90%\n",
            "Loss: 0.0011 Accuracy: 99.80%\n",
            "Loss: 0.0011 Accuracy2: 99.90%\n",
            "Loss: 0.0008 Accuracy: 99.80%\n",
            "Loss: 0.0008 Accuracy2: 99.90%\n",
            "Loss: 0.0091 Accuracy: 99.79%\n",
            "Loss: 0.0091 Accuracy2: 99.90%\n",
            "Loss: 0.0078 Accuracy: 99.79%\n",
            "Loss: 0.0078 Accuracy2: 99.89%\n",
            "Loss: 0.0006 Accuracy: 99.79%\n",
            "Loss: 0.0006 Accuracy2: 99.89%\n",
            "Loss: 0.0029 Accuracy: 99.79%\n",
            "Loss: 0.0029 Accuracy2: 99.89%\n",
            "Loss: 0.0006 Accuracy: 99.79%\n",
            "Loss: 0.0006 Accuracy2: 99.89%\n",
            "Loss: 0.0012 Accuracy: 99.79%\n",
            "Loss: 0.0012 Accuracy2: 99.90%\n",
            "Loss: 0.0003 Accuracy: 99.79%\n",
            "Loss: 0.0003 Accuracy2: 99.90%\n",
            "Loss: 0.0003 Accuracy: 99.79%\n",
            "Loss: 0.0003 Accuracy2: 99.90%\n",
            "Loss: 0.0046 Accuracy: 99.79%\n",
            "Loss: 0.0046 Accuracy2: 99.90%\n",
            "Loss: 0.0005 Accuracy: 99.79%\n",
            "Loss: 0.0005 Accuracy2: 99.90%\n",
            "Loss: 0.0059 Accuracy: 99.79%\n",
            "Loss: 0.0059 Accuracy2: 99.90%\n",
            "Loss: 0.0037 Accuracy: 99.79%\n",
            "Loss: 0.0037 Accuracy2: 99.90%\n",
            "Loss: 0.0038 Accuracy: 99.79%\n",
            "Loss: 0.0038 Accuracy2: 99.90%\n",
            "Loss: 0.0012 Accuracy: 99.80%\n",
            "Loss: 0.0012 Accuracy2: 99.90%\n",
            "Loss: 0.0009 Accuracy: 99.80%\n",
            "Loss: 0.0009 Accuracy2: 99.90%\n",
            "Loss: 0.0004 Accuracy: 99.80%\n",
            "Loss: 0.0004 Accuracy2: 99.90%\n",
            "Loss: 0.0019 Accuracy: 99.80%\n",
            "Loss: 0.0019 Accuracy2: 99.90%\n",
            "Loss: 0.0004 Accuracy: 99.80%\n",
            "Loss: 0.0004 Accuracy2: 99.90%\n",
            "Loss: 0.0045 Accuracy: 99.80%\n",
            "Loss: 0.0045 Accuracy2: 99.90%\n",
            "Loss: 0.0054 Accuracy: 99.80%\n",
            "Loss: 0.0054 Accuracy2: 99.90%\n",
            "Loss: 0.0013 Accuracy: 99.80%\n",
            "Loss: 0.0013 Accuracy2: 99.90%\n",
            "Loss: 0.0020 Accuracy: 99.80%\n",
            "Loss: 0.0020 Accuracy2: 99.90%\n",
            "Loss: 0.0017 Accuracy: 99.80%\n",
            "Loss: 0.0017 Accuracy2: 99.90%\n",
            "Loss: 0.0011 Accuracy: 99.80%\n",
            "Loss: 0.0011 Accuracy2: 99.90%\n",
            "Loss: 0.0019 Accuracy: 99.80%\n",
            "Loss: 0.0019 Accuracy2: 99.90%\n",
            "Loss: 0.0087 Accuracy: 99.80%\n",
            "Loss: 0.0087 Accuracy2: 99.90%\n",
            "Loss: 0.0001 Accuracy: 99.80%\n",
            "Loss: 0.0001 Accuracy2: 99.90%\n",
            "Loss: 0.0003 Accuracy: 99.81%\n",
            "Loss: 0.0003 Accuracy2: 99.90%\n",
            "Loss: 0.0005 Accuracy: 99.81%\n",
            "Loss: 0.0005 Accuracy2: 99.90%\n",
            "Loss: 0.0001 Accuracy: 99.81%\n",
            "Loss: 0.0001 Accuracy2: 99.90%\n",
            "Loss: 0.0007 Accuracy: 99.81%\n",
            "Loss: 0.0007 Accuracy2: 99.90%\n",
            "Loss: 0.0079 Accuracy: 99.80%\n",
            "Loss: 0.0079 Accuracy2: 99.90%\n",
            "Loss: 0.0010 Accuracy: 99.81%\n",
            "Loss: 0.0010 Accuracy2: 99.90%\n",
            "Loss: 0.0002 Accuracy: 99.81%\n",
            "Loss: 0.0002 Accuracy2: 99.90%\n",
            "Loss: 0.0002 Accuracy: 99.81%\n",
            "Loss: 0.0002 Accuracy2: 99.90%\n",
            "Loss: 0.0091 Accuracy: 99.81%\n",
            "Loss: 0.0091 Accuracy2: 99.90%\n",
            "Loss: 0.0101 Accuracy: 99.80%\n",
            "Loss: 0.0101 Accuracy2: 99.90%\n",
            "Loss: 0.0041 Accuracy: 99.80%\n",
            "Loss: 0.0041 Accuracy2: 99.90%\n",
            "Loss: 0.0009 Accuracy: 99.80%\n",
            "Loss: 0.0009 Accuracy2: 99.90%\n",
            "Loss: 0.0057 Accuracy: 99.80%\n",
            "Loss: 0.0057 Accuracy2: 99.90%\n",
            "Loss: 0.0023 Accuracy: 99.80%\n",
            "Loss: 0.0023 Accuracy2: 99.90%\n",
            "Loss: 0.0007 Accuracy: 99.81%\n",
            "Loss: 0.0007 Accuracy2: 99.90%\n",
            "Loss: 0.0023 Accuracy: 99.80%\n",
            "Loss: 0.0023 Accuracy2: 99.90%\n",
            "Loss: 0.0001 Accuracy: 99.81%\n",
            "Loss: 0.0001 Accuracy2: 99.90%\n",
            "Loss: 0.0034 Accuracy: 99.80%\n",
            "Loss: 0.0034 Accuracy2: 99.90%\n",
            "Loss: 0.0018 Accuracy: 99.80%\n",
            "Loss: 0.0018 Accuracy2: 99.90%\n",
            "Loss: 0.0013 Accuracy: 99.80%\n",
            "Loss: 0.0013 Accuracy2: 99.90%\n",
            "Loss: 0.0245 Accuracy: 99.80%\n",
            "Loss: 0.0245 Accuracy2: 99.90%\n",
            "Loss: 0.0062 Accuracy: 99.80%\n",
            "Loss: 0.0062 Accuracy2: 99.90%\n",
            "Loss: 0.0007 Accuracy: 99.80%\n",
            "Loss: 0.0007 Accuracy2: 99.90%\n",
            "Loss: 0.0012 Accuracy: 99.80%\n",
            "Loss: 0.0012 Accuracy2: 99.90%\n",
            "Loss: 0.0054 Accuracy: 99.79%\n",
            "Loss: 0.0054 Accuracy2: 99.90%\n",
            "Loss: 0.0026 Accuracy: 99.80%\n",
            "Loss: 0.0026 Accuracy2: 99.90%\n",
            "Loss: 0.0008 Accuracy: 99.80%\n",
            "Loss: 0.0008 Accuracy2: 99.90%\n",
            "Loss: 0.0037 Accuracy: 99.80%\n",
            "Loss: 0.0037 Accuracy2: 99.90%\n",
            "Loss: 0.0005 Accuracy: 99.80%\n",
            "Loss: 0.0005 Accuracy2: 99.90%\n",
            "Loss: 0.0002 Accuracy: 99.80%\n",
            "Loss: 0.0002 Accuracy2: 99.90%\n",
            "Final Test Accuracy: 99.80%\n",
            "Final Test Accuracy2: 99.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "concatenated_test_data = CIFAR10Dataset(testset)\n",
        "# 이어붙인 데이터셋으로 DataLoader 생성\n",
        "batch_size = 64\n",
        "test_loader = DataLoader(concatenated_test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "correct2 = 0\n",
        "total2 = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        pred_soft = torch.sigmoid(outputs)\n",
        "        pred_binary = pred_soft.gt(0.5).type(outputs.type())\n",
        "        new_labels = [[0.0 for tt in range(10)] for t in range(len(labels))]\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            new_labels[t][labels[t][j]] = 1.0\n",
        "        labels = torch.tensor(new_labels)\n",
        "        labels = labels.to(device)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total2 += (labels == labels).sum().item()\n",
        "        correct2 += (pred_binary == labels).sum().item()\n",
        "        for t in range(len(labels)):\n",
        "          for j in range(len(labels[t])):\n",
        "            if (labels[t][j] == 1.0):\n",
        "              total += 1\n",
        "              if (pred_binary[t][j] == 1.0):\n",
        "                correct += 1\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy: {(correct / total * 100):.2f}%') # Label이 1일 때, 추정값도 1인 개수 비율\n",
        "        print(f'Loss: {loss.item():.4f} Accuracy2: {(correct2 / total2 * 100):.2f}%') # 추정값 중 얼마나 맞았는지에 대한 비율 (Label과 추정값이 얼마나 같은지에 대한 비율)\n",
        "\n",
        "accuracy = correct / total\n",
        "accuracy2 = correct2 / total2\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%') # Label이 1일 때, 추정값도 1인 개수 비율\n",
        "print(f'Final Test Accuracy2: {accuracy2 * 100:.2f}%') # 추정값 중 얼마나 맞았는지에 대한 비율 (Label과 추정값이 얼마나 같은지에 대한 비율)"
      ],
      "metadata": {
        "id": "oBJLBQ0hrT1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9861b3bc-2d65-4342-d28e-08976ff82e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.4172 Accuracy: 93.06%\n",
            "Loss: 0.4172 Accuracy2: 96.41%\n",
            "Loss: 0.3131 Accuracy: 94.12%\n",
            "Loss: 0.3131 Accuracy2: 96.95%\n",
            "Loss: 0.0435 Accuracy: 95.53%\n",
            "Loss: 0.0435 Accuracy2: 97.50%\n",
            "Loss: 0.1098 Accuracy: 95.51%\n",
            "Loss: 0.1098 Accuracy2: 97.58%\n",
            "Loss: 0.3032 Accuracy: 95.16%\n",
            "Loss: 0.3032 Accuracy2: 97.38%\n",
            "Loss: 0.2119 Accuracy: 95.25%\n",
            "Loss: 0.2119 Accuracy2: 97.40%\n",
            "Loss: 0.2616 Accuracy: 94.99%\n",
            "Loss: 0.2616 Accuracy2: 97.23%\n",
            "Loss: 0.2568 Accuracy: 94.84%\n",
            "Loss: 0.2568 Accuracy2: 97.21%\n",
            "Loss: 0.2344 Accuracy: 95.01%\n",
            "Loss: 0.2344 Accuracy2: 97.22%\n",
            "Loss: 0.4210 Accuracy: 94.83%\n",
            "Loss: 0.4210 Accuracy2: 97.06%\n",
            "Loss: 0.3365 Accuracy: 94.82%\n",
            "Loss: 0.3365 Accuracy2: 97.09%\n",
            "Loss: 0.2903 Accuracy: 94.89%\n",
            "Loss: 0.2903 Accuracy2: 97.10%\n",
            "Loss: 0.2222 Accuracy: 94.85%\n",
            "Loss: 0.2222 Accuracy2: 97.09%\n",
            "Loss: 0.2175 Accuracy: 94.86%\n",
            "Loss: 0.2175 Accuracy2: 97.14%\n",
            "Loss: 0.1513 Accuracy: 94.97%\n",
            "Loss: 0.1513 Accuracy2: 97.21%\n",
            "Loss: 0.3398 Accuracy: 94.98%\n",
            "Loss: 0.3398 Accuracy2: 97.16%\n",
            "Loss: 0.3007 Accuracy: 94.99%\n",
            "Loss: 0.3007 Accuracy2: 97.10%\n",
            "Loss: 0.2635 Accuracy: 95.10%\n",
            "Loss: 0.2635 Accuracy2: 97.17%\n",
            "Loss: 0.4645 Accuracy: 95.03%\n",
            "Loss: 0.4645 Accuracy2: 97.11%\n",
            "Loss: 0.2617 Accuracy: 95.03%\n",
            "Loss: 0.2617 Accuracy2: 97.15%\n",
            "Loss: 0.2150 Accuracy: 95.05%\n",
            "Loss: 0.2150 Accuracy2: 97.16%\n",
            "Loss: 0.0904 Accuracy: 95.07%\n",
            "Loss: 0.0904 Accuracy2: 97.19%\n",
            "Loss: 0.3413 Accuracy: 95.02%\n",
            "Loss: 0.3413 Accuracy2: 97.17%\n",
            "Loss: 0.1485 Accuracy: 95.06%\n",
            "Loss: 0.1485 Accuracy2: 97.19%\n",
            "Loss: 0.3893 Accuracy: 95.06%\n",
            "Loss: 0.3893 Accuracy2: 97.17%\n",
            "Loss: 0.2707 Accuracy: 95.06%\n",
            "Loss: 0.2707 Accuracy2: 97.16%\n",
            "Loss: 0.1032 Accuracy: 95.11%\n",
            "Loss: 0.1032 Accuracy2: 97.21%\n",
            "Loss: 0.3282 Accuracy: 95.14%\n",
            "Loss: 0.3282 Accuracy2: 97.22%\n",
            "Loss: 0.3402 Accuracy: 95.08%\n",
            "Loss: 0.3402 Accuracy2: 97.15%\n",
            "Loss: 0.3120 Accuracy: 95.04%\n",
            "Loss: 0.3120 Accuracy2: 97.13%\n",
            "Loss: 0.3407 Accuracy: 95.06%\n",
            "Loss: 0.3407 Accuracy2: 97.14%\n",
            "Loss: 0.2059 Accuracy: 95.11%\n",
            "Loss: 0.2059 Accuracy2: 97.18%\n",
            "Loss: 0.2453 Accuracy: 95.08%\n",
            "Loss: 0.2453 Accuracy2: 97.17%\n",
            "Loss: 0.1580 Accuracy: 95.11%\n",
            "Loss: 0.1580 Accuracy2: 97.20%\n",
            "Loss: 0.2450 Accuracy: 95.14%\n",
            "Loss: 0.2450 Accuracy2: 97.21%\n",
            "Loss: 0.2427 Accuracy: 95.11%\n",
            "Loss: 0.2427 Accuracy2: 97.21%\n",
            "Loss: 0.4209 Accuracy: 95.05%\n",
            "Loss: 0.4209 Accuracy2: 97.19%\n",
            "Loss: 0.0875 Accuracy: 95.11%\n",
            "Loss: 0.0875 Accuracy2: 97.23%\n",
            "Loss: 0.1631 Accuracy: 95.14%\n",
            "Loss: 0.1631 Accuracy2: 97.25%\n",
            "Loss: 0.0005 Accuracy: 95.15%\n",
            "Loss: 0.0005 Accuracy2: 97.25%\n",
            "Final Test Accuracy: 95.15%\n",
            "Final Test Accuracy2: 97.25%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}